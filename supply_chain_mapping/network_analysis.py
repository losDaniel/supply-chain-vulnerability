import os
from path import Path 
root = str(Path(os.path.abspath(os.path.dirname(__file__))).parent)

import supply_chain_mapping.data_cleaning_and_processing as dc

import pandas as pd
import numpy as np
import networkx as nx
import random, copy


random.seed(1)


def sample_network(graph_data, 
                   sample_by_percentage=None, 
                   sample_by_year=False, 
                   sample_by_slaughter=False, 
                   sample_by_origin=False, 
                   min_date=None,
                   max_date=None):
    '''Take a sample of the network movement to identify nodes and edges to build the network with. 
    __________
    parameters
    - sample_by_percentage : 0 < flaot < 1. Takes a random sample of this % size from the total IDs in the dataset of movements. This can be applied in combinationwith one other sampling method to take a random sample of IDs in that sample.  
    - sample_by_year : bool, default False. If True, only uses movements after min_date and before max_date  
    - sample_by_slaughter : bool, default False. If True, collected the IDs of cows slaughtered after min_date and before max_date and use all of their movements to map the network 
    - sample_by_origin : bool, default False. If True, collected the IDs of cows whose journey began after min_date and before max_date and use all of their movements to map the network 
    - min_date : str in format '01/01/2020', default None. 
    - max_date : str in format '01/01/2020', default None.     
    '''

    if sample_by_year or sample_by_slaughter or sample_by_origin:
        assert min_date is not None 
        assert max_date is not None 
    else: 
        try: assert sample_by_percentage 
        except Exception as e: 
            print('Must select at least one sampling method')
            raise e 
        movements = graph_data # If we're not sampling using dates 

    if sample_by_year:     
        try:
            assert sample_by_slaughter is False 
            assert sample_by_origin is False
        except Exception as e: 
            print('More than one date-based sampling method cannot be used simultaneously')
            raise e 
        # Sample the movements based on whether they were moved within a given year        
        # We use the destination date to track movement because all destinations indicate movement within the network (eventually to slaughter)
        # whereas the date of origin includes birth which is a non-movement across the network 
        movements = graph_data[((graph_data['date_destination']>min_date) & (graph_data['date_destination']<max_date))]
        
    if sample_by_slaughter:
        try:
            assert sample_by_year is False 
            assert sample_by_origin is False 
        except Exception as e: 
            print('More than one date-based sampling method cannot be used simultaneously')
            raise e 
        # keep the dates of slaughters and check the dates of slaughter fall within range 
        movements = graph_data[graph_data['status_destination']=='Slaughter']
        movements = movements[(movements['date_destination']>min_date) & (movements['date_destination']<max_date)]
        # If sampling by slaughter or origin we are using IDs and not movements for sampling 
        ids_to_use = list(movements['Individual Identification  Number'].unique()) 
        movements = graph_data[graph_data['Individual Identification  Number'].isin(ids_to_use)] # Keep all the movements of the cows with the selected IDs 
    
    if sample_by_origin:
        try:
            assert sample_by_year is False 
            assert sample_by_slaughter is False 
        except Exception as e: 
            print('More than one date-based sampling method cannot be used simultaneously')
            raise e
        # Sort by cow and date and keep the first obs for each cow.         
        movements = graph_data.sort_values(['Individual Identification  Number','date_origin']).groupby('Individual Identification  Number', as_index=False).first()
        movements = movements[(movements['date_destination']>min_date) & (movements['date_destination']<max_date)]
        # If sampling by slaughter or origin we are using IDs and not movements for sampling 
        ids_to_use = list(movements['Individual Identification  Number'].unique()) 
        movements = graph_data[graph_data['Individual Identification  Number'].isin(ids_to_use)] # Keep all the movements of the cows with the selected IDs 
    
    if sample_by_percentage is not None:
        # Take a sample of the data. Default is the full dataset  # clocked 1.46s 
        num_unique_ids = len(list(movements['Individual Identification  Number'].unique()))
        ids_to_use = random.sample(list(movements['Individual Identification  Number'].unique()), int(np.floor(num_unique_ids*sample_by_percentage)))
        movements = movements[movements['Individual Identification  Number'].isin(ids_to_use)]
        
    return movements 


def get_networks(graph_data, directed=True):
    
    # Create a dataset of origin attributes for each node 
    summ_cols = ['date_origin','duration'] + list(graph_data['status_origin'].unique())
    node_origin_attributes = pd.concat([graph_data, pd.get_dummies(graph_data['status_origin'])], 1)[['origin']+summ_cols].groupby('origin').mean()
    
    # Create a dataset of origin attributes for each node 
    summ_cols = ['date_destination'] + list(graph_data['status_destination'].unique())
    node_destination_attributes = pd.concat([graph_data, pd.get_dummies(graph_data['status_destination'])], 1)[['destination']+summ_cols].groupby('destination').mean()
    
    # Merge the origin and destination atrributes for each node 
    node_info = node_origin_attributes.merge(node_destination_attributes, right_index=True, left_index=True, indicator=True, suffixes=('_org','_dst'))
        
    # Collapse the network based on origin-destination connections to get the edges of the networks 
    graph_data['weight'] = 1 
    edges = graph_data.groupby(['origin','destination'],as_index=False)['weight'].sum()
    edges['weight'] = edges['weight']/edges['weight'].max()
    descriptive_stats = {} 
    for stat in edges['weight'].describe().__dict__['_index']:
        if stat not in descriptive_stats:
            descriptive_stats[stat]=[]

        descriptive_stats[stat].append(edges['weight'].describe()[stat])
    #print('Weight distribution parameters:') 
    #print(edges['weight'].describe())
    
    
    if directed: 
        Gn = nx.DiGraph() 
    else: 
        Gn = nx.Graph() 
    
    # Transpose the node-info 
    node_info_T = node_info.transpose()
    
    attr=node_info_T.index.values    
    
    for col in node_info_T.columns:
        attr_node=dict(list(zip(attr, node_info_T[col].values)))
        Gn.add_node(col,attr_dic=attr_node)
    
    # Read in the edges for the network 
    for tup in edges.itertuples():
        Gn.add_edge(tup.origin, tup.destination, weight=tup.weight)
        
    return Gn, descriptive_stats


def get_graphs_by_year(graph_data, years:list, directed=False, years_to_use=1, use_list_interval=True):
    '''
    Create separate graphs sampling movements from the graph_data based on min and max dates defined by the elements in the years list.
    If the years in the list skip a year (i.e. [2002, 2004, 2006,...]) then each network will be based on the sample of movements for two years at a time. 

    Parameters
    ----------
    graph_data : pandas.DataFrame()
        Date with all the movements in the network.
    years : list of ints. 
        list of years (integers).
    directed : bool, default False. 
        If true, return directed graph objects. 
    years_to_use : int, default 1. 
        When the elements of the list do not define the max_date for a sample add +`years_to_use` years to the min_date to define the max_date. 
    use_list_interval : bool, default True. 
        If True the min_date and max_date for each sample will be determined by consecutive elements in the years list. 
        If False the min_date will be each element in the list and the max_date will be min_date + `years_to_use` years. 

    Returns
    -------
    network_by_year : list
        DESCRIPTION.
    directed_graphs_by_year : dict. 
        Dictionary with the networkx graph object for each year. 
    '''
    years.sort()

    n_nodes, n_edges = [], [] 

    stat_data = {} 
    directed_graphs_by_year = {} 
    for i, y in enumerate(years): 
        min_date = '01/01/'+str(int(y))
        if use_list_interval:
            j = i + 1 
            try: 
                # Try to get the next year on the list
                max_date = '01/01/'+str(int(years[j]))
            except: 
                # If it is not available simple set the max date as one year after the last one
                max_date = '01/01/'+str(int(years[i])+years_to_use)
        else: 
            max_date = '01/01/'+str(int(years[i])+years_to_use)

        sample_data = sample_network(graph_data, sample_by_year=True, min_date=min_date, max_date=max_date)
        directed_graph, descriptive_stats = get_networks(sample_data, directed = directed)

        # Store the directed graphs so we can continue running tests. 
        directed_graphs_by_year[int(y)] = directed_graph

        for key in descriptive_stats.keys():
            if key not in stat_data: 
                stat_data[key]=[]
            stat_data[key]+=descriptive_stats[key]

        n_nodes.append(directed_graph.number_of_nodes())
        n_edges.append(directed_graph.number_of_edges())
    
    network_by_year = pd.concat([pd.DataFrame({'Year':[int(y) for y in years],'NumNodes':n_nodes,'NumEdges':n_edges}), pd.DataFrame(stat_data)], 1)
    
    return network_by_year, directed_graphs_by_year




def check_node_consistency(directed_graphs, graph_names):
    # This is a square matrix of year v. year 
    data = {} 
    for y in graph_names: data[int(y)] = [] 
    mat_index = [] 

    # Loop through each row 
    for _a in graph_names: 
        # Set the indices for this row, each row has three levels 
        mat_index.append((_a, 'A not B'))
        mat_index.append((_a, 'A and B'))
        mat_index.append((_a, 'B not A'))

        yga = directed_graphs[_a]

        # For each row, loop through each column and attach the necessary values 
        for _b in graph_names: 
            # Don't work through the same year 
            if _a == _b: 
                a_not_b = np.nan
                a_and_b = np.nan
                b_not_a = np.nan
            else: 
                ygb = directed_graphs[_b]

                seta = set(yga.nodes)
                setb = set(ygb.nodes)
                a_not_b = len(seta.difference(setb))
                a_and_b = len(seta.intersection(setb))
                b_not_a = len(setb.difference(seta))

            data[_b].append(a_not_b)
            data[_b].append(a_and_b)
            data[_b].append(b_not_a)

    df = pd.DataFrame(data, index=pd.MultiIndex.from_tuples(mat_index))        
    return df


def check_node_persistence_by_year(base_year:int, comparison_years:list, directed_graphs_by_year):
    
    assert base_year in directed_graphs_by_year
    for y in comparison_years: assert y in directed_graphs_by_year
    
    original_set = set(directed_graphs_by_year[base_year].nodes)
    base_set = copy.deepcopy(original_set)
    
    for year in comparison_years: 
        comp_set = set(directed_graphs_by_year[year].nodes)
        base_set = base_set.intersection(comp_set)

    print(len(base_set),'of', len(original_set), 'nodes in the base year', base_year,'were present in all the comparison years', ','.join([str(y) for y in comparison_years]))        
    
    return base_set


def get_empirical_network_properties(gs, shortest_path=False, largest_component=True, verbose=False):
    # network property ---------
    
    # empirical network model
    ## number of nodes N
    n_node = gs.number_of_nodes()
    ## number of link K
    n_edge = gs.number_of_edges()
    ## ave. clustring coef <C>
    c = nx.average_clustering(gs)
    ## ave. degree <K>
    k = np.mean(list(dict(gs.degree()).values()))
    if verbose:     
        print("============empirical network model===================")
        print("Number of nodes = ",n_node)
        print("Number of links = ",n_edge)
        print("Average clustering coefficient of the network = ",c)
        print("Average degree of the network = ",k)
    ## ave. shortest path <L>
    disconnected = -99
    if shortest_path: 
        try:
            l = nx.average_shortest_path_length(gs)
            disconnected = 0
        except:

            largest_component = sorted((gs.subgraph(c) for c in nx.connected_components(gs)), key = len, reverse=True)[0]
            l = nx.average_shortest_path_length(largest_component)
            disconnected = 1 
        if verbose: 
            print("Average shortest path = ", l)

    return n_node, n_edge, c, k, l, disconnected


def get_wattsstrogatz_network_properties(gs, n_edge, n_node, c, default_rewire_prob=0.5, shortest_path=False, largest_component=True, verbose=False):
    # small-world model a.k.a. Watts-Strogatz model
    k = int(n_edge/n_node)*2
    C0 = nx.average_clustering(nx.watts_strogatz_graph(n_node,k, 0), count_zeros=True)
    ## probability of rewiring each edge
    try: 
        p_s = 1-pow(c/C0,1/3)
    except: 
        p_s = default_rewire_prob
    gs=nx.watts_strogatz_graph(n_node,k,p_s,seed=123)
    
    ## number of nodes N
    n_node_s=gs.number_of_nodes()
    ## number of link K
    n_edge_s=gs.number_of_edges()
    ## ave. clustring coef <C>
    c_s = nx.average_clustering(gs)
    ## ave. degree <K>
    k_s = np.mean(list(dict(gs.degree()).values()))
    ## ave. shortest path <L>
    if verbose: 
        print("============small-world network model===================")
        print("p = ",p_s)
        print("Number of nodes = ",n_node_s)
        print("Number of links = ",n_edge_s)
        print("Average clustering coefficient of the network = ",c_s)
        print("Average degree of the network = ",k_s)
    l_s = np.NaN
    disconnected = -99
    if shortest_path: 
        try:
            l_s = nx.average_shortest_path_length(gs)
            disconnected = 0
        except:

            largest_component = sorted((gs.subgraph(c) for c in nx.connected_components(gs)), key = len, reverse=True)[0]
            l_s = nx.average_shortest_path_length(largest_component)
            disconnected = 1 
        if verbose: 
            print("Average shortest path = ", l_s)

    return n_node_s, n_edge_s, c_s, k_s, l_s, disconnected, p_s


def get_barabasialbert_network_propeties(gs, m_b, n_node, shortest_path=False, largest_component=True, verbose=False):

    #Barabasi-Albert network model
    ## Set m. I totally forgot how to decide m. possibly, set m as the # of edges is close to the actual network...
    gb = nx.barabasi_albert_graph(n_node, m_b,seed=123)
    ## number of nodes N
    n_node_b=gb.number_of_nodes()
    ## number of link K
    n_edge_b=gb.number_of_edges()
    ## ave. clustring coef <C>
    c_b = nx.average_clustering(gb)
    ## ave. degree <K>
    k_b = np.mean(list(dict(gb.degree()).values()))
    ## ave. shortest path <L>
    if verbose: 
        print("============Barabasi-Albert network model===================")
        print("m = ",m_b)
        print("Number of nodes = ",n_node_b)
        print("Number of links = ",n_edge_b)
        print("Average clustering coefficient of the network = ",c_b)
        print("Average degree of the network = ",k_b)
    l_b = np.NaN
    disconnected = -99
    if shortest_path: 
        try:
            l_b = nx.average_shortest_path_length(gs)
            disconnected = 0
        except:

            largest_component = sorted((gs.subgraph(c) for c in nx.connected_components(gs)), key = len, reverse=True)[0]
            l_b = nx.average_shortest_path_length(largest_component)
            disconnected = 1 
        if verbose: 
            print("Average shortest path = ", l_b)

    return n_node_b, n_edge_b, c_b, k_b, l_b, disconnected
            
    
def get_erdosrenyi_network_properties(gs, n_node, n_edge, shortest_path=False, largest_component=True, verbose=False):
    
    # random graph, a.k.a Erdős-Rényi graph
    ## Probability for edge creation
    p_e = 2*n_edge/(n_node*(n_node-1))
    ge = nx.erdos_renyi_graph(n_node, p_e, seed=123, directed=True)
    #CC= sorted((ge.subgraph(c) for c in nx.connected_components(ge)), key = len, reverse=True)[0]
    ## number of nodes N
    n_node_e=ge.number_of_nodes()
    ## number of link K
    n_edge_e=ge.number_of_edges()
    ## ave. clustring coef <C>
    c_e = nx.average_clustering(ge)
    ## ave. degree <K>
    k_e = np.mean(list(dict(ge.degree()).values()))
    if verbose: 
        print("============Erdős-Rényi network model===================")
        print("p = ",p_e)
        print("Number of nodes = ",n_node_e)
        print("Number of links = ",n_edge_e)
        print("Average clustering coefficient of the network = ",c_e)
        print("Average degree of the network = ",k_e)
    ## ave. shortest path <L> choose either of below functions
    l_e = np.NaN
    disconnected = -99
    if shortest_path: 
        try:
            l_e = nx.average_shortest_path_length(gs)
            disconnected = 0
        except:

            largest_component = sorted((gs.subgraph(c) for c in nx.connected_components(gs)), key = len, reverse=True)[0]
            l_e = nx.average_shortest_path_length(largest_component)
            disconnected = 1 
        if verbose: 
            print("Average shortest path = ", l_e)
        
    return n_node_e, n_edge_e, c_e, k_e, l_e, disconnected, p_e