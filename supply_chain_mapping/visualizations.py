import os
from path import Path 
root = str(Path(os.path.abspath(os.path.dirname(__file__))).parent)

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

import supply_chain_mapping.data_cleaning_and_processing as dc
from supply_chain_mapping import network_analysis as na

import re 
import random
import pandas as pd
import  numpy  as  np
from  japanmap  import  picture
from vcue.basics import remove_values_from_list
from vcue.data import order

import networkx as nx

import  matplotlib.pyplot  as  plt 
from  pylab  import  rcParams


def plot_scraper_entropy(factor, complete_data):
    
    num_edges, num_nodes, num_ids = dc._get_scraper_entropy_data(factor, complete_data)
    # Create traces
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=num_ids, y=num_edges,
                        mode='lines',
                        name='# edges'))
    fig.add_trace(go.Scatter(x=num_ids, y=num_nodes,
                        mode='lines',
                        name='# nodes'))
    
    fig.update_layout(title='Number of Edges & Nodes by IDs Scraped')
    
    fig.show()


def _plot_on_map(summary, slaughter_summary):
    '''Plot data on the map'''
    japan_summary = summary[["PrefJ","node","Prefecture"]].merge(slaughter_summary, on='Prefecture', how='inner')
    
    
    japan_summary['Cow_P'] = japan_summary['#Cows']/japan_summary['#Cows'].sum()
    
    perf_list = japan_summary['PrefJ'].values
    rate = japan_summary['Cow_P'].values
    rf = rate.astype(np.float32)

    def color_scale(r): 
        return(0, int(255 - 255/np.max(rf) * r), 255)
    
    data = {} 
    for p, r in zip(perf_list, rf): 
        c  =  color_scale(r) 
        data[p] = c
    
    rcParams['figure.figsize'] = 14, 14 
    plt.imshow(picture (data));

def plot_ids_by_prefecture(summary, title='Scraped IDs by Prefecture',y='node',x='Prefecture',text='#Cows'):
    '''Plots a bar graph showing the number of cows that started their grim journey in each prefecture'''
    fig = px.bar(summary, y=y, x=x, text=text)
    fig.update_traces(texttemplate='%{text:.2s}', textposition='outside', marker_color='orange')
    fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title=title)
    fig.show()
    
    
def plot_nodes_ids_by_prefecture(summary, title='# of Nodes & Cows by Prefecture'):    
    '''Plots a double bar graph with number of '''
    x = summary['Prefecture'].values
    y1 = summary['node'].values
    y2 = summary['#Cows'].values
    
    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=x,
        y=y1,
        name='Nodes',
        marker_color='indianred',
    ))
    fig.add_trace(go.Bar(
        x=x,
        y=y2,
        name='#Cows',
        marker_color='lightsalmon',
    ))
    
    # Here we modify the tickangle of the xaxis, resulting in rotated labels.
    fig.update_layout(barmode='group', xaxis_tickangle=-45, title=title)
    #fig.write_image(project_path+'/documents/# of Nodes & Cows by Prefecture', format='pdf', engine='kaleido')
    #fig.write_html(project_path+'/documents/# of Nodes & Cows by Prefecture.html')
    fig.show()




def elbow_plot_inertia(wss, clusters, highlight):
    # Using elbow method to select the correct number of clusters    
    _ = plt.figure(figsize = (10,7))
    _ = plt.plot(clusters, wss, linewidth = 2, color = 'blue', marker='+', markersize = 8)
    _ = plt.title('Elbow Method', fontsize = 12)
    _ = plt.xlabel('Number of Clusters',fontsize = 10)
    _ = plt.ylabel('Inertia',fontsize = 10)
    
    n_clusters = highlight
    
    _ = plt.axvline(x = n_clusters, linewidth = 2, color = 'red', linestyle = '--')
    _ = plt.show()



def graph_node_consistency_by_year(df, years, years_to_use=1, use_list_interval=True):
    
    vanishing_nodes, persistent_nodes, incoming_nodes = [], [], []  
    for i, year in enumerate(years): 
        
        if use_list_interval:
            j = i + 1 
            try: 
                # Try to get the next year on the list
                next_year = int(years[j])
            except: 
                # If it is not available simple set the max date as one year after the last one
                next_year = int(years[i])+years_to_use
        else: 
            next_year = int(years[i])+years_to_use

        try: 
            vanishing_nodes.append(df.loc[(year,'A not B'), next_year])
            persistent_nodes.append(df.loc[(year,'A and B'), next_year])
            incoming_nodes.append(df.loc[(year,'B not A'), next_year])
        except: 
            pass

    visuals = pd.DataFrame({'In Network: Vanishing Nodes':vanishing_nodes,'In Network: Persistent Nodes':persistent_nodes,'Incoming Nodes':incoming_nodes}, index=years[:-1]) 
    visuals = pd.DataFrame(visuals.stack()).reset_index()
    visuals.columns = ['Year','Detection','Value']


    fig = px.area(visuals, x="Year", y="Value", color="Detection",
                  line_group="Detection")
    fig.update_layout(
        title=go.layout.Title(
            text="Network Consistency Year over Year<br><sup>Incoming and persistent nodes are the nodes IN a network any given year.<br>Incoming nodes replace vanishing nodes in our sample each year.</sup>",
            xref="paper",
            x=0
        ),
    )
    fig.show()


    
def plot_degrees_out_in_directedG(g):

    out_d=list(dict(g.out_degree()).values())
    in_d=list(dict(g.in_degree()).values())
    
    ######NEED to eliminate ZEROS
    in_d=remove_values_from_list(in_d,0)
    out_d=remove_values_from_list(out_d,0)
    
    out_weighted=list(dict(g.out_degree(weight='weight')).values())
    in_weighted=list(dict(g.in_degree(weight='weight')).values())
    ######NEED to elkminate ZEROS
    out_weighted=remove_values_from_list(out_weighted,0)
    in_weighted=remove_values_from_list(in_weighted,0)
    
    fig, ax = plt.subplots()
    fig.set_size_inches((9, 7))
    
    n_bins = 20
    
    #n, bins = np.histogram(out_d, bins = range(min(out_d), max(out_d)+1, 2), normed="True") 
    out_logBins = np.logspace(np.log10(min(out_d)), np.log10(max(out_d)),num=n_bins)
    out_logBinDensity, out_binedges = np.histogram(out_d, bins=out_logBins, density=True)
    
    #n, bins = np.histogram(in_d, bins = range(min(in_d), max(in_d)+1, 2), normed="True") 
    in_logBins = np.logspace(np.log10(min(in_d)), np.log10(max(in_d)),num=n_bins)
    in_logBinDensity, in_binedges = np.histogram(in_d, bins=in_logBins, density=True)
    
    ax.loglog(out_logBins[:-1],out_logBinDensity,'o', markersize=10,label=r'$k_{in}$')
    ax.loglog(in_logBins[:-1],in_logBinDensity,'s', markersize=10,label=r'$k_{out}$')
    ax.legend(fontsize=30)
    
    
    ax.set_xlabel('$degree, k$',fontsize=40)
    ax.set_ylabel('$P(k)$',fontsize=40) 
    plt.savefig("distributions.eps",dpi=200,bbox_inches='tight')
    
    


def _vector_result_data(results_data, var):
    
    em_data = results_data.loc['empirical network'][[var]].values
    em_data = em_data.reshape(len(em_data),)

    sw_index = [idx for idx in results_data.index if 'small world' in idx]
    sw_data = results_data.loc[sw_index][[var]].values
    sw_data = sw_data.reshape(len(sw_data),)
    
    ba_index = [idx for idx in results_data.index if 'Barabasi-Albert' in idx]
    ba_data = results_data.loc[ba_index][[var]].values
    ba_data = ba_data.reshape(len(ba_data),)
    
    er_index = [idx for idx in results_data.index if 'Erdős-Rényi' in idx]
    er_data = results_data.loc[er_index][[var]].values
    er_data = er_data.reshape(len(er_data),)
    
    return em_data, sw_data, ba_data, er_data

def plot_model_fit_over_time(results_data):
    cols_to_graph = ['# of link','<C>','<K>','<L>']
    
    fig = make_subplots(rows=2, cols=2)
    
    years = list(results_data['year'].unique())#.astype(str).unique())
    
    on_col = True 
    row = 1 
    col = 1 
    for var in cols_to_graph:
        
        em_data, sw_data, ba_data, er_data = _vector_result_data(results_data, var)
    
        # Create traces
        fig.add_trace(go.Scatter(x=years, y=em_data,
                            mode='lines',
                            name='empirical_'+var,
                            line = dict(color='royalblue', width=4, dash='solid')),
                     row=row, col=col)
    
        fig.add_trace(go.Scatter(x=years, y=sw_data,
                            mode='lines',
                            name='small world_'+var,
                            line = dict(color='lightgreen', width=4, dash='solid')),
                     row=row, col=col)
    
        fig.add_trace(go.Scatter(x=years, y=ba_data,
                            mode='lines',
                            name='barabasi-albert_'+var,
                            line = dict(color='darksalmon', width=4, dash='solid')),
                     row=row, col=col)
    
        fig.add_trace(go.Scatter(x=years, y=er_data,
                            mode='lines', name='erdos renyi_'+var,
                            line = dict(color='maroon', width=4, dash='solid')),
                     row=row, col=col)
        if on_col: 
            col+=1
            on_col=False
        else: 
            row+=1
            col=1
            on_col=True
    
    fig.update_layout(height=800, width=1600, title_text="Network Attributes Over Time")
    
    return fig

def loglogplot_model_degree_distribution(empirical_graphs, ws_graphs, ba_graphs, er_graphs, year, save_as="./networkplot.png"):

    #G1:empirical
    degs0 = list(dict(nx.degree(empirical_graphs[year])).values())
    n0, bins0 = np.histogram(degs0, bins = list(range(min(degs0), max(degs0)+1, 1)), density="True")
    
    #Gs:Small World
    degs1 = list(dict(nx.degree(ws_graphs[year])).values())
    n1, bins1 = np.histogram(degs1, bins = list(range(min(degs1), max(degs1)+1, 1)), density="True")
    
    #Gb:Barabasi Albert
    degs2 = list(dict(nx.degree(ba_graphs[year])).values())
    n2, bins2 = np.histogram(degs2, bins = list(range(min(degs2), max(degs2)+1, 1)), density="True")
    
    #Ge:Erdos Renyi
    degs3 = list(dict(nx.degree(er_graphs[year])).values())
    n3, bins3 = np.histogram(degs3, bins = list(range(min(degs3), max(degs3)+1, 1)), density="True")
    
    plt.figure(figsize=(17,8)) #use once and set figure size
    plt.loglog(bins0[:-1],n0,'b-', markersize=10, label="Empirical Data") 
    plt.loglog(bins1[:-1],n1,'bs--', markersize=10, label="Small World") 
    plt.loglog(bins2[:-1],n2,'go--', markersize=10, label="Barabasi Albert") 
    plt.loglog(bins3[:-1],n3,'r*--', markersize=10, label="Erdos Renyi")
    plt.legend(loc='upper right',prop={'size': 30})
    plt.title('Degree Distributions log-log plot',fontsize=30,y=1.1)
    plt.xlabel('Degree, k',fontsize=30)
    plt.ylabel('P(k)',fontsize=30)
    plt.xticks(fontsize=30)
    plt.yticks(fontsize=30)
    plt.tight_layout()
    plt.savefig(save_as)
    plt.show;


def graph_community_attributes_over_time(graphs_by_slaughter_year, years, louvain = True,  k_clique_3 = True, louvain_data=None, k_clique_data=None): #k_clique_2 = False,
        
    if (louvain) and (louvain_data is None): 
        louvain_data, louvain_c = na.communities_over_time(graphs_by_slaughter_year, years, louvain=True, k_clique=False)
        louvain_data = louvain_data.rename(columns={'max':'Largest Community', 'mean':'Mean Size', '50%': 'Median Size'})
    # if k_clique_2:
    #     k_clique_data_2, k_clique_c_2 = na.communities_over_time(graphs_by_slaughter_year, years, louvain=False, k_clique=True, k_clique_min_connections=2)
    if (k_clique_3) and (k_clique_data is None):
        k_clique_data_3, k_clique_c_3 = na.communities_over_time(graphs_by_slaughter_year, years, louvain=False, k_clique=True, k_clique_min_connections=3)
        k_clique_data_3 = k_clique_data_3.rename(columns={'max':'Largest Community', 'mean':'Mean Size', '50%': 'Median Size'})
        
    x_axis = list(louvain_data.index)
        
    #on_col = True 
    row = 1 
    col = 1 
    
    #cols_to_graph = ['Total # Comm','% Nodes in Comms','max','50%','mean','std']
    cols_to_graph = ['Total # Comm','Largest Community',('Mean Size','Median Size')]
    
    fig = make_subplots(rows=3, cols=1, subplot_titles=tuple([', '.join(list(i)) if type(i)==tuple else str(i) for i in cols_to_graph ]))
    
    showlegend=True
    
    for i, var in enumerate(cols_to_graph):
        
        if type(var) != tuple: 
            if louvain:
                louvain_vals = list(louvain_data[var].values)
        
                # Create traces
                fig.add_trace(go.Scatter(x=x_axis, y=louvain_vals,
                                    mode='lines',
                                    name='Louvain:<br>'+var,
                                    line = dict(color='maroon', width=4, dash='solid'),
                                    legendgroup = str(i),
                                    showlegend=showlegend),
                             row=row, col=col)
        
            if k_clique_3:
                k_clique_vals_3 = list(k_clique_data_3[var].values)
        
                fig.add_trace(go.Scatter(x=x_axis, y=k_clique_vals_3,
                                    mode='lines',
                                    name='K(3)-Clique:<br>'+var,
                                    line = dict(color='royalblue', width=4, dash='solid'),
                                    legendgroup = str(i),
                                    showlegend=showlegend),
                             row=row, col=col)
            
            if showlegend:
                showlegend=True

        else: 
            for j,v in enumerate(var): 
                colors1 = ['maroon','darksalmon']
                colors2 = ['royalblue','lightsteelblue']
                if j == 1: showlegend=True
                if louvain:
                    louvain_vals = list(louvain_data[v].values)
                    # Create traces
                    fig.add_trace(go.Scatter(x=x_axis, y=louvain_vals,
                                        mode='lines',
                                        name='Louvain:<br>'+v,
                                        line = dict(color=colors1[j], width=4, dash='solid'),
                                        legendgroup = str(i),
                                        showlegend=showlegend),
                                 row=row, col=col)
            
                if k_clique_3:
                    k_clique_vals_3 = list(k_clique_data_3[v].values)
            
                    fig.add_trace(go.Scatter(x=x_axis, y=k_clique_vals_3,
                                        mode='lines',
                                        name='K(3)-Clique:<br>'+v,
                                        line = dict(color=colors2[j], width=4, dash='solid'),
                                        legendgroup = str(i),
                                        showlegend=showlegend),
                                 row=row, col=col)            
                    
        row+=1            
            
        # if k_clique_2:
        #     k_clique_vals_2 = list(k_clique_data_2[var].values)
    
        #     fig.add_trace(go.Scatter(x=x_axis, y=k_clique_vals_2,
        #                         mode='lines',
        #                         name='K(2)-Clique :'+var),
        #                  row=row, col=col)
    
        # if on_col: 
        #     col+=1
        #     on_col=False
        # else: 
        #     row+=1
        #     col=1
        #     on_col=True
        
    fig.update_layout(height=800, width=1000, title_text="Community Attributes Over Time", legend_tracegroupgap = 180,)    
    fig.update_annotations(font_size=10)    
    # https://plotly.com/python/reference/layout/#layout-legend-groupclick
    fig.update_layout(legend_groupclick='toggleitem')
    #https://plotly.com/python/reference/layout/#layout-legend-x
    fig.update_layout(legend_font_size=10)    
    #fig.update_layout(legend_x=0)
    #fig.update_layout(legend_orientation='h')
    
    #fig.update_layout(legend_xanchor='left')   
    #fig.update_layout(legend_yanchor='top')
    # fig.update_layout(
    # legend=dict(
    #     x=0,
    #     y=1,
    #     traceorder="reversed",
    #     title_font_family="Times New Roman",
    #     font=dict(
    #         family="Courier",
    #         size=12,
    #         color="black"
    #     ),
    #     bgcolor="LightSteelBlue",
    #     bordercolor="Black",
    #     borderwidth=2
    # )
    # )
    
    if k_clique_3 and louvain:
        return fig, louvain_data, k_clique_data_3, louvain_c, k_clique_c_3
    elif k_clique_3 and not louvain:
        return fig, k_clique_data_3, k_clique_c_3
    elif louvain and not k_clique_3:
        return fig, louvain_data, louvain_c
        

from datetime import datetime

def plot_network_ages(s_network_age, sample_means):
    
    s_network_age = order(s_network_age, 'sample_year').sort_values('sample_year',ascending=False)
    
    s_net_plot = s_network_age.drop('sample_year',1)
    s_net_plot['max_col'] = s_net_plot.max(axis=1)
    for c in s_net_plot.columns:
        s_net_plot[c] = s_net_plot[c]/s_net_plot['max_col']
    s_net_plot = s_net_plot.drop('max_col',1)
    
    plot_years = sorted([datetime(i[0],i[1],1) for i in s_net_plot.columns])
    s_net_plot = order(s_net_plot, [(d.year,d.month) for d in plot_years])
    
    fig = make_subplots(rows=2, cols=1, row_heights=[0.2, 0.8], shared_xaxes=True,)
    
    #fig = go.FigureWidget()
    
    years = sorted(list(sample_means.index))
    fig.add_trace(go.Scatter(
        x = years, 
        y = list(sample_means['Mean Age'].values),
        mode='lines',
        line = dict(color='mediumvioletred', width=4, dash='solid')),
        row=1,
        col=1
    )
    
    # y_upper = list(sample_means['y_top'].values)
    # y_lower = list(sample_means['y_bottom'].values)
    
    # fig.add_trace(go.Scatter(
    #     x=years+years[::-1], # x, then x reversed
    #     y=y_upper+y_lower[::-1], # upper, then lower reversed
    #     fill='toself',
    #     fillcolor='rgba(0,100,80,0.2)',
    #     line=dict(color='rgba(255,255,255,0)'),
    #     hoverinfo="skip",
    #     showlegend=False
    # ))
    
    fig.add_trace(go.Heatmap(
        z=s_net_plot.fillna(0),
        x=plot_years,
        y=s_network_age['sample_year'].values,
        text = s_network_age.drop('sample_year',1),
        colorscale='Agsunset',
        zmin=0,
        zmax=0.5),
        row=2,
        col=1
    )
    
    fig.update_layout(
        title="Age of Network Data by Sample Year",
        #xaxis_title="network built using movements sampled from this year",
    #    yaxis_title="sample year",
        legend_title="Legend Title",
        font=dict(
            family="Courier New, monospace",
            size=12,
            color="RebeccaPurple"
        )
    )
    
    fig.update_yaxes(title_text="sample year", row=2, col=1)
    fig.update_yaxes(title_text="mean age (days)", row=1, col=1)
    fig.update_xaxes(title_text="network built using movements sampled from this year", row=2, col=1)
    
    #return fig 
    return fig 


def plot_network_robustness(year_graph, attack_specificity = 10, plot=True):     
    #Random failure
    C = year_graph.copy()
    N = year_graph.number_of_nodes()
    number_of_bins = 50
    M_1 = N // number_of_bins
    num_nodes_removed_1 = range(0, N, M_1)
    
    random_attack_core_proportions = []
    for nodes_removed in num_nodes_removed_1:
        C = year_graph.copy()
        if C.number_of_nodes() > nodes_removed:
            nodes_to_remove = random.sample(list(C.nodes), nodes_removed)
            C.remove_nodes_from(nodes_to_remove)
        # Measure the relative size of the network core
            core = max(nx.connected_components(C))
            core_proportion = len(core) / N
            random_attack_core_proportions.append(core_proportion)    
            
    # Targeted attack
    C = year_graph.copy()
    N = year_graph.number_of_nodes()
    specify = attack_specificity 
    number_of_bins = number_of_bins * specify
    M_2 = N // number_of_bins
    num_nodes_removed = range(0, N, M_2)
    
    targeted_attack_core_proportions = []
    for nodes_removed in num_nodes_removed:
        C = year_graph.copy()
        if C.number_of_nodes() > nodes_removed:
            nodes_sorted_by_degree = sorted(C.nodes, key=C.degree, reverse=True)
            nodes_to_remove = nodes_sorted_by_degree[:nodes_removed]
            C.remove_nodes_from(nodes_to_remove)
            # Measure the relative size of the network core
            core = max(nx.connected_components(C))
            core_proportion = len(core) / N
            targeted_attack_core_proportions.append(core_proportion)
            
    min_random_nodes = min([j*M_1 for j,a in enumerate(random_attack_core_proportions) if a<0.01])
    print('Earliest system failure from random attacks occurred at',min_random_nodes,'nodes (specificity',str(M_1)+')')
    
    min_attack_nodes = min([j*M_2 for j,a in enumerate(targeted_attack_core_proportions) if a<0.01])
    print('Earliest system failure from targetted attacks occurred at',min_attack_nodes,'nodes (specificity',str(M_2)+')')
    
    graph_attacks = targeted_attack_core_proportions[::specify]
    while len(graph_attacks) > len(num_nodes_removed_1):
        graph_attacks = graph_attacks[:-1]
    
    if plot: 
        plt.title('Random Failure vs. Targeted Attack at Day0')
        plt.xlabel('Number of nodes removed')
        plt.ylabel('Proportion of nodes in core')
        plt.plot(num_nodes_removed_1, random_attack_core_proportions, marker='o', label='Random')
        plt.plot(num_nodes_removed_1, graph_attacks, marker='^', label='Targeted')
        plt.legend()

    return min_random_nodes, min_attack_nodes

def _prep_tidy_db(node_data, cluster_variables):
    # Index db on cluster ID
    tidy_db = node_data.set_index('Cluster')
    tidy_db = tidy_db[cluster_variables]
    tidy_db = tidy_db.stack()
    tidy_db = tidy_db.reset_index()
    tidy_db = tidy_db.rename(columns={
                            'level_1': 'Variable', 
                            0: 'Values'})
    
    # for var in list(tidy_db['Variable'].unique()):
    #     tidy_db.loc[tidy_db['Variable']==var, 'Values'] = tidy_db.loc[tidy_db['Variable']==var, 'Values']/tidy_db.loc[tidy_db['Variable']==var, 'Values'].max()
    #     print(var)
    #     print(tidy_db.loc[tidy_db['Variable']==var, 'Values'].describe())
        
    #tidy_db['Values'] = tidy_db['Values'].abs()
    #tidy_db
    return tidy_db


def plot_var_cluster_dists(tidy_db, cluster_focus=None, colors= None ):

    if colors is None: 
        colors = ['lightseagreen','darkorange','lawngreen','purple','red','yellow','grey','brown','cerulian']    

    if cluster_focus is not None: 
        grafics = tidy_db[tidy_db['Cluster'].isin(cluster_focus)]
    else: 
        grafics = tidy_db
    
    color = {} 
    for i, clus in enumerate(list(grafics['Cluster'].unique())):
        color[clus] = colors[i]
    
    # Setup the facets
    import seaborn 
    #color = {'norm_duration':"lightseagreen", 'p_move_in':"darkorange", 'p_max_n_moves':"lawngreen"}#, 3:"purple", 4:"red", 5:"pink"}
    #color = {'logn_duration':"lightseagreen", 'logn_p_move_in':"darkorange", 'logn_max_n_moves':"lawngreen"}#, 3:"purple", 4:"red", 5:"pink"}
    
    facets = seaborn.FacetGrid(data=grafics, col='Variable', palette=color, hue='Cluster', 
                      sharey=False, sharex=False, aspect=3, col_wrap=2)
    
    for j in range(1,10,2):
        i = j/10
        print('Log of',i,'=',np.log(i))
    _ = facets.map(seaborn.kdeplot, 'Values', shade=True).add_legend()



def show_cluster_dist(node_data):
    pd.DataFrame(node_data['Cluster'].value_counts().sort_index()).reset_index().rename(columns={'index':'Cluster','Cluster':'Count'})

    fig = px.bar(node_data, x='year', y='pop',
                 hover_data=['lifeExp', 'gdpPercap'], color='lifeExp',
                 labels={'pop':'population of Canada'}, height=400)
    fig.show()
#~
def import_plotly_color_scales(shuffle=True):
    
    from_white_scales = ['Blues','BuGn','BuPu','GnBu','Greens','Greys','PuBu','PuBuGn','Purples','RdPu','amp','tempo']

    return from_white_scales

def import_plotly_color_list(shuffle=True):
    
    color_list = """aliceblue, antiquewhite, aqua, aquamarine, azure,
            beige, bisque, black, blanchedalmond, blue,
            blueviolet, brown, burlywood, cadetblue,
            chartreuse, chocolate, coral, cornflowerblue,
            cornsilk, crimson, cyan, darkblue, darkcyan,
            darkgoldenrod, darkgray, darkgrey, darkgreen,
            darkkhaki, darkmagenta, darkolivegreen, darkorange,
            darkorchid, darkred, darksalmon, darkseagreen,
            darkslateblue, darkslategray, darkslategrey,
            darkturquoise, darkviolet, deeppink, deepskyblue,
            dimgray, dimgrey, dodgerblue, firebrick,
            floralwhite, forestgreen, fuchsia, gainsboro,
            ghostwhite, gold, goldenrod, gray, grey, green,
            greenyellow, honeydew, hotpink, indianred, indigo,
            ivory, khaki, lavender, lavenderblush, lawngreen,
            lemonchiffon, lightblue, lightcoral, lightcyan,
            lightgoldenrodyellow, lightgray, lightgrey,
            lightgreen, lightpink, lightsalmon, lightseagreen,
            lightskyblue, lightslategray, lightslategrey,
            lightsteelblue, lightyellow, lime, limegreen,
            linen, magenta, maroon, mediumaquamarine,
            mediumblue, mediumorchid, mediumpurple,
            mediumseagreen, mediumslateblue, mediumspringgreen,
            mediumturquoise, mediumvioletred, midnightblue,
            mintcream, mistyrose, moccasin, navajowhite, navy,
            oldlace, olive, olivedrab, orange, orangered,
            orchid, palegoldenrod, palegreen, paleturquoise,
            palevioletred, papayawhip, peachpuff, peru, pink,
            plum, powderblue, purple, red, rosybrown,
            royalblue, rebeccapurple, saddlebrown, salmon,
            sandybrown, seagreen, seashell, sienna, silver,
            skyblue, slateblue, slategray, slategrey, snow,
            springgreen, steelblue, tan, teal, thistle, tomato,
            turquoise, violet, wheat, white, whitesmoke,
            yellow, yellowgreen"""
            
    parsed_colors = re.findall('([a-z]*)', color_list)
    parsed_colors = [c for c in parsed_colors if c!='']
    
    if shuffle: 
        import random
        random.shuffle(parsed_colors)
        parsed_colors
    
    return parsed_colors
