import os
from path import Path 
root = str(Path(os.path.abspath(os.path.dirname(__file__))).parent)

import copy
import pandas as pd 
from random import sample 
from vcue.data import order
from IPython.display import clear_output


def _load_complete_data(root=root, rename=False):
    complete_data = pd.read_csv(root+'/data/collected/complete_collected_save.csv')
    if rename: complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    return complete_data


def _get_jdict(root=root):
    '''Get the translations of Japanese Prefectures'''
    japan_translate = pd.read_csv(root+'/data/japanese_prefectures.csv')
    jdict = japan_translate[['Prefecture','PrefJ']]
    return jdict 


def _add_prefecture(complete_data=None, jdict=None, root=root): 
    '''Get a summar of the data by prefecture'''    
    if complete_data is None: 
        complete_data = _load_complete_data(root=root)
    else: 
        assert type(complete_data)==pd.core.frame.DataFrame
    if jdict is None: 
        jdict = _get_jdict(root=root)
    
    # Format date column
    complete_data['異動年月日'] = complete_data['異動年月日'].str.replace('-','')
    complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
    
    complete_data['year'] = pd.DatetimeIndex(complete_data['異動年月日']).year
    
    complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    
    # Get the number of cows by prefecture 
    gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['PrefJ'].first()
    gd1['Individual Identification  Number'] = gd1['Individual Identification  Number'].astype(str)

    # Get the number of nodes (locations) by prefecture    
    gd2 = complete_data.groupby('node', as_index=False)['PrefJ'].first()
    gd2 = gd2.groupby('PrefJ').count().reset_index()
    
    summary = gd1.groupby('PrefJ').count()
    summary = summary.merge(gd2, on='PrefJ', how='inner')
    summary = summary.reset_index().merge(jdict, on='PrefJ', how='outer', indicator=True)
    summary = summary.rename(columns={'Individual Identification  Number':'#Cows','_merge':'merged'})
    summary['Cow_P'] = summary['#Cows']/summary['#Cows'].sum()
    summary = summary[summary['#Cows'].notnull()]
    summary['node_P'] = summary['node']/summary['node'].sum()
    
    return summary


def summary_by_prefecture(complete_data=None,jdict=None): 
    summary = _add_prefecture(complete_data, jdict)
    return summary         
    
    
def summary_by_year(first=True, complete_data=None, root=root):
    if complete_data is None: 
        complete_data= _load_complete_data(root=root)

    try:
        complete_data['異動年月日'] = complete_data['異動年月日'].str.replace('-','')
        complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
        complete_data['year'] = pd.DatetimeIndex(complete_data['異動年月日']).year
    except: 
        pass
    
        
    print('First date in the data:',complete_data['異動年月日'].dropna().min())
    print('Last date in the data:',complete_data['異動年月日'].max())
    
    complete_data['year'] = complete_data[complete_data['year'].notnull()]['year'].astype(int).astype(str)
    if first: 
        gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['year'].first()
    else: 
        gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['year'].last()
    gd1['Individual Identification  Number'] = gd1['Individual Identification  Number'].astype(str)
    summary = gd1.groupby('year').count()
    summary.columns = ['#Cows']
    
    return summary.sort_index(ascending=False).transpose()
    

status_keys = {
    '出生':'Birth', 
    '転出':'Move-out', 
    '搬入':'Carrying-in', 
    '取引':'Sale', 
    '転入':'Move-in', 
    '搬出':'Carrying-out', 
    'と畜':'Slaughter', 
    '装着':'Equip', 
    '死亡':'Death', 
    '装着または転入':'Equip Or Transfer',
    '輸入':'Import'}

    
def summarize_slaughtered_supplychain(first=True, complete_data=None, jdict=None, root=root, status_keys=status_keys):
    '''Get the summary data, only for the cows that made it to slaughter (i.e. not counting cows that dies prematurely or are still in the chain)'''  
    if complete_data is None: 
        complete_data = _load_complete_data(root=root, rename=True)
    if jdict is None: 
        jdict = _get_jdict(root=root)
    
    complete_data['status'] = complete_data['異動内容'].map(status_keys)
    slaughtered = complete_data[complete_data['status']=='Slaughter']
    
    slaughter_summary = slaughtered.groupby('PrefJ', as_index=False)[['Individual Identification  Number']].count()
    prefdict = {}
    for i, row in jdict.iterrows():
        prefdict[row['PrefJ']]=row['Prefecture']
    slaughter_summary['PrefJ'] = slaughter_summary['PrefJ'].map(prefdict)    
    slaughter_summary.columns = ['Prefecture','#Cows']
    
    return slaughter_summary, slaughtered
    
  
def get_network_data(complete_data=None):

    slaughter_summary, slaughtered = summarize_slaughtered_supplychain()

    if complete_data is None: 
        complete_data = _load_complete_data(rename=True)
    else: 
        complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    
    complete_data['status'] = complete_data['異動内容'].map(status_keys)
    
    # Format date column
    complete_data['異動年月日'] = complete_data['異動年月日'].str.replace('-','')
    complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
    
    complete_slaughter = complete_data.merge(slaughtered[['Individual Identification  Number']], on='Individual Identification  Number', how='inner', indicator=True)
    complete_slaughter = complete_slaughter.drop('_merge',1)
    complete_slaughter = complete_slaughter.sort_values(['Individual Identification  Number', '異動年月日']) # sort by ID and Date
    
    complete_slaughter = complete_slaughter.rename(columns={'異動年月日':'date_destination', 'node':'destination','status':'status_destination','PrefJ':'Pref_destination'})
    
    complete_slaughter['origin'] = complete_slaughter.groupby('Individual Identification  Number')['destination'].shift(1)
    complete_slaughter['status_origin'] = complete_slaughter.groupby('Individual Identification  Number')['status_destination'].shift(1)
    complete_slaughter['date_origin'] = complete_slaughter.groupby('Individual Identification  Number')['date_destination'].shift(1)
    complete_slaughter['Pref_origin'] = complete_slaughter.groupby('Individual Identification  Number')['Pref_destination'].shift(1)
    
    complete_slaughter['date_origin'] = pd.to_datetime(complete_slaughter['date_origin'])
    complete_slaughter['date_destination'] = pd.to_datetime(complete_slaughter['date_destination'])
    complete_slaughter['duration'] = complete_slaughter['date_destination'] - complete_slaughter['date_origin']
    complete_slaughter['duration'] = complete_slaughter['duration'].apply(lambda d: d.days)
    complete_slaughter['age'] = complete_slaughter.groupby('Individual Identification  Number')['duration'].cumsum()
    
    complete_slaughter = order(complete_slaughter, ['Individual Identification  Number','origin','status_origin','date_origin','Pref_origin','duration','age','destination','status_destination','date_destination','Pref_destination'])
    
    # Create a dataset which can summarize the end of each cow's trajectory
    # only_slaughter = complete_slaughter.groupby('Individual Identification  Number', as_index=False).last()
    
    # Create a dataset which can summarize the beginning of each cow's trajectory 
    # only_birth = complete_slaughter.sort_values(['Individual Identification  Number','date_destination']).groupby('Individual Identification  Number', as_index=False).nth(0)
    
    # Verify that the first origin for each observation is missing 
    # print(only_birth['origin'].value_counts())
    
    complete_slaughter['movement_number'] = complete_slaughter.groupby('Individual Identification  Number').cumcount()
    graph_data = complete_slaughter[complete_slaughter['movement_number']>0]
    
    #graph_data['edges'] = graph_data['origin'].astype(str)+'_'+graph_data['destination'].astype(str)
    
    return graph_data 
    
def _get_scraper_entropy_data(factor, complete_data):

    num_edges = [] 
    num_nodes = [] 
    num_ids = [] 
    
    df = copy.deepcopy(complete_data)
    collected = pd.DataFrame()
    pool = list(df['Individual Identification  Number'].values)
    index_length = 0
    counter = 1 
    while len(pool)>0:  
        try: 
            sample_index = sample(list(pool),factor)
        except: # If the sampling fails its because the size has been reduced to 0 and we can stop 
            pool = [] 
            continue
        
        index_length+=len(sample_index)

        collected = collected.append(df[df['Individual Identification  Number'].isin(sample_index)])
        clear_output()
        print('On sample:', counter)
        print('Accounting for:', index_length,' IDs')
        print(len(pool), " remaining") 
        graph_data = get_network_data(collected)
    
        # Number of unique edges in the network 
        num_edges.append(len(graph_data.groupby(['origin','destination'])['movement_number'].count()))
    
        # Number of unique nodes in the network 
        num_nodes.append(len(set(list(graph_data['origin'].unique())+list(graph_data['destination'].unique()))))
    
        num_ids.append(factor*counter)
        
        df = df.loc[list(set(df.index).difference(set(collected.index)))]
        pool = list(df['Individual Identification  Number'].values)
        
        counter+=1 
        
    clear_output()

    return num_edges, num_nodes, num_ids