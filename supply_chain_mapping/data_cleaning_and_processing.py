import os
from path import Path 
root = str(Path(os.path.abspath(os.path.dirname(__file__))).parent)

import copy
import pandas as pd 
import numpy as np
from random import sample 
from vcue.data import order
from IPython.display import clear_output


def _load_complete_data(root=root, rename=False):
    complete_data = pd.read_csv(root+'/data/collected/complete_collected_save.csv')
    if rename: complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    return complete_data


def _get_jdict(root=root):
    '''Get the translations of Japanese Prefectures'''
    japan_translate = pd.read_csv(root+'/data/japanese_prefectures.csv')
    jdict = japan_translate[['Prefecture','PrefJ']]
    return jdict 


def _add_prefecture(complete_data=None, jdict=None, root=root): 
    '''Get a summar of the data by prefecture'''    
    if complete_data is None: 
        complete_data = _load_complete_data(root=root)
    else: 
        assert type(complete_data)==pd.core.frame.DataFrame
    if jdict is None: 
        jdict = _get_jdict(root=root)
    
    # Format date column
    complete_data['異動年月日'] = complete_data['異動年月日'].str.replace('-','')
    complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
    
    complete_data['year'] = pd.DatetimeIndex(complete_data['異動年月日']).year
    
    complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    
    # Get the number of cows by prefecture 
    gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['PrefJ'].first()
    gd1['Individual Identification  Number'] = gd1['Individual Identification  Number'].astype(str)

    # Get the number of nodes (locations) by prefecture    
    gd2 = complete_data.groupby('node', as_index=False)['PrefJ'].first()
    gd2 = gd2.groupby('PrefJ').count().reset_index()
    
    summary = gd1.groupby('PrefJ').count()
    summary = summary.merge(gd2, on='PrefJ', how='inner')
    summary = summary.reset_index().merge(jdict, on='PrefJ', how='outer', indicator=True)
    summary = summary.rename(columns={'Individual Identification  Number':'#Cows','_merge':'merged'})
    summary['Cow_P'] = summary['#Cows']/summary['#Cows'].sum()
    summary = summary[summary['#Cows'].notnull()]
    summary['node_P'] = summary['node']/summary['node'].sum()
    
    return summary


def summary_by_prefecture(complete_data=None,jdict=None): 
    summary = _add_prefecture(complete_data, jdict)
    return summary         
    
    
def summary_by_year(first=True, complete_data=None, root=root):
    if complete_data is None: 
        complete_data= _load_complete_data(root=root)

    try:
        complete_data['異動年月日'] = complete_data['異動年月日'].str.replace('-','')
        complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
        complete_data['year'] = pd.DatetimeIndex(complete_data['異動年月日']).year
    except: 
        pass
    
        
    print('First date in the data:',complete_data['異動年月日'].dropna().min())
    print('Last date in the data:',complete_data['異動年月日'].max())
    
    complete_data['year'] = complete_data[complete_data['year'].notnull()]['year'].astype(int).astype(str)
    if first: 
        gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['year'].first()
    else: 
        gd1 = complete_data.groupby('Individual Identification  Number', as_index=False)['year'].last()
    gd1['Individual Identification  Number'] = gd1['Individual Identification  Number'].astype(str)
    summary = gd1.groupby('year').count()
    summary.columns = ['#Cows']
    
    return summary.sort_index(ascending=False).transpose()
    

status_keys = {
    '出生':'Birth', 
    '転出':'Move-out', 
    '搬入':'Carrying-in', 
    '取引':'Sale', 
    '転入':'Move-in', 
    '搬出':'Carrying-out', 
    'と畜':'Slaughter', 
    '装着':'Equip', 
    '死亡':'Death', 
    '装着または転入':'Equip Or Transfer',
    '輸入':'Import'}

    
def summarize_slaughtered_supplychain(first=True, complete_data=None, jdict=None, root=root, status_keys=status_keys):
    '''Get the summary data, only for the cows that made it to slaughter (i.e. not counting cows that dies prematurely or are still in the chain)'''  
    if complete_data is None: 
        complete_data = _load_complete_data(root=root, rename=True)
    if jdict is None: 
        jdict = _get_jdict(root=root)
    
    complete_data['status'] = complete_data['異動内容'].map(status_keys)
    slaughtered = complete_data[complete_data['status']=='Slaughter']
    
    slaughter_summary = slaughtered.groupby('PrefJ', as_index=False)[['Individual Identification  Number']].count()
    prefdict = {}
    for i, row in jdict.iterrows():
        prefdict[row['PrefJ']]=row['Prefecture']
    slaughter_summary['PrefJ'] = slaughter_summary['PrefJ'].map(prefdict)    
    slaughter_summary.columns = ['Prefecture','#Cows']
    
    return slaughter_summary, slaughtered
    
  
def get_network_data(complete_data=None, only_slaughtered=True):
    '''
    Get horizontal network data with information on the origin and destination node for each cows movements
    Updates: this function automatically excludes movements where the origin and destination node are the same by converting the original complete data (which is organized by movements) into a dataset organized by legs of their journey.

    Parameters
    ----------
    complete_data : pandas.DataFrame, optional
        The default is None and will load default complete data dataset in the data foler
    only_slaughtered : bool, optional
        The default is True. If True will only consider supply chain for cows who were eventually slaughtered. Cows who have not been slaughtered are probably still alive in the system. 

    Returns
    -------
    graph_data : pandas.DataFrame. 
        A dataset that can be simplified and submitted to a network graph object

    '''

    if complete_data is None: 
        complete_data = _load_complete_data(rename=True)
    else: 
        complete_data = complete_data.rename(columns={'都道府県':'PrefJ','氏名または名称':'node'})
    
    leg_data, activities = get_leg_data(complete_data, only_slaughtered=only_slaughtered)
    # We don't need all the indicator columns 
    leg_data = leg_data.drop(activities, 1)
    
    # We do want to highlight the important activities that took place in each spot. If the first activity at a node was birth that goes down. 
    # Otherwise we higlight the last activity. Thus, places with births could also slaughter cows and we would not see the slaughter. 
    # But places that move-out were transitionary points and places with slaughter or sale served that specific purpose.
    # By using this status variable, every node that is a destination will not get a birth status but a transitory or terminating status. 
    # The data you see in this variable will depend on whether the `only_slaughtered` option was selected
    leg_data['critical_status'] = np.where(leg_data['status_first']=='Birth', 'Birth', leg_data['status_last'])
    
    complete_slaughter = leg_data.sort_values(['Individual Identification  Number', 'date_min'])
    
    # Using leg data excludes the status variables because it is meant to be used with clusters  
    complete_slaughter = complete_slaughter.rename(columns={'date_min':'date_destination', 'node':'destination','critical_status':'status_destination','PrefJ':'Pref_destination'})
    
    complete_slaughter['origin'] = complete_slaughter.groupby('Individual Identification  Number')['destination'].shift(1)
    complete_slaughter['status_origin'] = complete_slaughter.groupby('Individual Identification  Number')['status_destination'].shift(1)
    complete_slaughter['date_origin'] = complete_slaughter.groupby('Individual Identification  Number')['date_destination'].shift(1)
    complete_slaughter['Pref_origin'] = complete_slaughter.groupby('Individual Identification  Number')['Pref_destination'].shift(1)
    
    # The date (origin or destination) is when the cow arrived at the node 
    complete_slaughter['date_origin'] = pd.to_datetime(complete_slaughter['date_origin'])
    complete_slaughter['date_destination'] = pd.to_datetime(complete_slaughter['date_destination'])
    complete_slaughter['duration'] = complete_slaughter['date_destination'] - complete_slaughter['date_origin']
    complete_slaughter['duration'] = complete_slaughter['duration'].apply(lambda d: d.days)
    complete_slaughter['age'] = complete_slaughter.groupby('Individual Identification  Number')['duration'].cumsum()
    
    #complete_slaughter = order(complete_slaughter, ['Individual Identification  Number','origin','status_origin','date_origin','Pref_origin','duration','age','destination','status_destination','date_destination','Pref_destination'])
    complete_slaughter = order(complete_slaughter, ['Individual Identification  Number','origin','date_origin','Pref_origin','duration','age','destination','date_destination','Pref_destination'])
        
    complete_slaughter['movement_number'] = complete_slaughter.groupby('Individual Identification  Number').cumcount()
    graph_data = complete_slaughter[complete_slaughter['movement_number']>0]
    
    return graph_data 

    
def _get_scraper_entropy_data(factor, complete_data):

    num_edges = [] 
    num_nodes = [] 
    num_ids = [] 
    
    df = copy.deepcopy(complete_data)
    collected = pd.DataFrame()
    pool = list(df['Individual Identification  Number'].values)
    index_length = 0
    counter = 1 
    while len(pool)>0:  
        try: 
            sample_index = sample(list(pool),factor)
        except: # If the sampling fails its because the size has been reduced to 0 and we can stop 
            pool = [] 
            continue
        
        index_length+=len(sample_index)

        collected = collected.append(df[df['Individual Identification  Number'].isin(sample_index)])
        clear_output()
        print('On sample:', counter)
        print('Accounting for:', index_length,' IDs')
        print(len(pool), " remaining") 
        graph_data = get_network_data(collected)
    
        # Number of unique edges in the network 
        num_edges.append(len(graph_data.groupby(['origin','destination'])['movement_number'].count()))
    
        # Number of unique nodes in the network 
        num_nodes.append(len(set(list(graph_data['origin'].unique())+list(graph_data['destination'].unique()))))
    
        num_ids.append(factor*counter)
        
        df = df.loc[list(set(df.index).difference(set(collected.index)))]
        pool = list(df['Individual Identification  Number'].values)
        
        counter+=1 
        
    clear_output()

    return num_edges, num_nodes, num_ids

def retrieve_node_summary_data(graph_data):

    origins = graph_data[['origin','status_origin','date_origin','Pref_origin','duration']]
    origins.columns = ['node','status','date','PrefJ','duration']
    
    destinations = graph_data[['destination','status_destination','date_destination','Pref_destination','age']]
    destinations.columns = ['node','status','date','PrefJ','age']
    
    node_data = origins.append(destinations)
    
    # Get a list of unique activities 
    activities = list(node_data['status'].unique())
    
    node_activity = pd.concat([node_data, pd.get_dummies(node_data['status'])], 1)
    
    agg_dict = dict(zip(activities,[['mean']]*len(activities)))
    agg_dict['date'] = ['min','max', 'count'] 
    agg_dict['PrefJ'] = ['first']
    
    node_activity = node_activity.groupby('node').agg(agg_dict)
    
    # Get the active window for any given node 
    node_activity['active_window'] = node_activity[('date','max')] - node_activity[('date','min')]
        
    return node_activity 


def get_leg_data(complete_data, only_slaughtered=True): 
    
    complete_data['status'] = complete_data['異動内容'].map(status_keys)
    complete_data = complete_data.drop('異動内容',1)
    
    # Format date column
    complete_data['異動年月日'] = complete_data['異動年月日'].astype(str).str.replace('-','')
    complete_data['異動年月日'] = pd.to_datetime(complete_data['異動年月日'])
    complete_data = complete_data.rename(columns={'異動年月日':'date'})
    
    complete_data = complete_data.rename(columns={'市区町村':'Municipalities'})
    
    complete_data = pd.concat([complete_data, pd.get_dummies(complete_data['status'])], 1)
    
    complete_data = complete_data.sort_values(['Individual Identification  Number','date'])
    
    # Create an indicator for movements across nodes for each cow 
    complete_data['previous_node'] = complete_data.groupby('Individual Identification  Number')['node'].shift(1)
    complete_data['moved_into_new_node'] = np.where(complete_data['node']!=complete_data['previous_node'], 1, 0)
    
    # Create a cumulative sum of times they moved across nodes. 
    # Each observation for each cow will now have a number showing which leg of their journey (which number node) they're in
    # If a cow returns to a previous node from a different node its a new leg 
    complete_data['leg_of_jouney'] = complete_data.groupby('Individual Identification  Number')['moved_into_new_node'].cumsum()
    #complete_data.head(50)
    
    if only_slaughtered:
        # Drop cows who were never slaughtered 
        complete_data['slaughtered'] = complete_data.groupby('Individual Identification  Number')['Slaughter'].transform('max')
        complete_data = complete_data[complete_data['slaughtered']==1]
        complete_data = complete_data.drop('slaughtered', 1 )
    
    # Get node summary data by leg of journey 
    # By getting the activities that take place in each leg of the journey we capture all the activities 
    # that take place in a given node without diluting the place's activity with multiple observations 
    activities = list(complete_data['status'].unique())
    
    agg_dict = dict(zip(activities, [['max']]*len(activities)))
    agg_dict['PrefJ'] = ['first']
    agg_dict['Municipalities'] = ['first']
    agg_dict['node'] = ['first']
    agg_dict['date'] = ['min','max','count']    
    agg_dict['status'] = ['first','last']

    leg_data = complete_data.groupby(['Individual Identification  Number','leg_of_jouney'],as_index=False).agg(agg_dict)
    
    leg_data['leg_duration'] = leg_data[('date','max')] - leg_data[('date','min')]
    leg_data['leg_duration'] = leg_data['leg_duration'].apply(lambda d: d.days)
    # leg_data = leg_data.drop([('date','max'), ('date','min')], 1)
    leg_data.columns = [c[0]  if (c[0]!='date') and (c[0]!='status') else c[0]+'_'+c[1] for c in leg_data.columns]
    leg_data = leg_data.rename(columns={'date_count':'count'})
    
    return leg_data, activities

def get_node_data(leg_data, activities):
    summ_dict = dict(zip(activities, [['mean']]*len(activities)))
    summ_dict['leg_duration'] = ['mean']
    summ_dict['count'] = ['count']
    summ_dict['PrefJ'] = ['first']
    summ_dict['Municipalities'] = ['first']
    
    node_data = leg_data.groupby('node', as_index=False).agg(summ_dict)
    node_data['leg_duration'] = node_data['leg_duration'].fillna(0)
    node_data.columns = [c[0] for c in node_data.columns]
    
    return node_data