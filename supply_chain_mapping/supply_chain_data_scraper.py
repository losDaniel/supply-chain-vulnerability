import time, copy 
import numpy as np 
import pandas as pd
from vcue import web 
from bs4 import BeautifulSoup


def _handle_save(failures, final_data, complete_data, _saves, name='recovery', return_names=False):
    '''A short internal function to save files'''
    failures.to_csv('failure_'+name+'_save'+str(_saves)+'.csv')
    final_data.to_csv('final_'+name+'_save'+str(_saves)+'.csv')
    complete_data.to_csv('complete_'+name+'_save'+str(_saves)+'.csv')
    if return_names: 
        return 'failure_'+name+'_save'+str(_saves)+'.csv', 'final_'+name+'_save'+str(_saves)+'.csv', 'complete_'+name+'_save'+str(_saves)+'.csv'
    

def _append_temporary_files(filename_tuple_list):
    '''Append and accumulate all the temporary data files'''
    failures, final_data, complete_data = pd.DataFrame({'ID':[],'Failure Code':[]}), pd.DataFrame(), pd.DataFrame()

    for f_tuple in filename_tuple_list: 
        f1, f2, f3 = f_tuple
        
        d1 = pd.read_csv(f1) # failure 
        d2 = pd.read_csv(f2) # final 
        d3 = pd.read_csv(f3) # complete 

        failures = failures.append(d1)        
        final_data = final_data.append(d2)
        complete_data = complete_data.append(d3)

    return failures, final_data, complete_data        

    
def _remove_scraped(master_list, scraped_list):
    '''Remove items in the scraped_list from the master list'''
    seta = set(master_list)
    setb = set(scraped_list)
    cleaned = seta.difference(setb)
    return list(cleaned)


def _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, name='temporary', return_names=True):
    '''An internal function to save temporary files and remove them from an on-going list'''
    # Save the data so far in a temporary file and add it to a list of files so they can be appended again later
    temp_files_saved += _handle_save(failures, final_data, complete_data, temp_counter, name='temporary', return_names=True)

    # Reset all the datasets so we can clear cached memory 
    failures, final_data, complete_data = pd.DataFrame({'ID':[],'Failure Code':[]}), pd.DataFrame(), pd.DataFrame()
    temp_counter += 1 

    # Remove the IDs that have been scraped so we don't repeat work
    master_list = _remove_scraped(master_list, scraped)                    
    
    return failures, final_data, complete_data, temp_counter, master_list



def supply_chain_data_scraper(samples : list, japanese = True, split_save = 0):
    '''Submit a list of numbers (ids) that can be used to identify cows in on the site "https://www.id.nlbc.go.jp/CattleSearch/search/agreement_en"
    __________
    parameters
    - samples : list of str. Each element should be an ID to be scraped. 
    - japanese : bool, default True. If true scrape the japanese website, if false use the english site. The English site displays less detailed information. 
    - split_save : int. Save the data in progress after this many ids have been scraped. This should be a rather high number, the point at which holding the data in memory becomes expensive in terms of time 
    '''
    
    # Create a deep copy of the master list because we might be forced to modify it and we don't want to affect the original 
    master_list = copy.deepcopy(samples)    
    samples_to_loop_through = copy.deepcopy(samples)  # Create a live list of the samples we need to loop through at any given time 
    scraped = [] # Create a list to store the samples we have scraped (and found or not found)
    
    # Temporary stored data counter, used for files that are recovered after and error and files that are saved using the split_save functionality 
    temp_counter = 1 

    # If the scraper fails completely 10 times then close it and return the error 
    attempts = 0 

    if split_save > 0: 
        record_number = split_save
        temp_files_saved = []

    while attempts < 10: 
        attempts +=1 
        try: 
            # Launch a Selenium web driver 
            driver = web.launch_driver()
            
            # Create a dataset to record failures and store them separately from the data
            failures = pd.DataFrame({'ID':[],'Failure Code':[]}) 

            # Create a dataset to store the summary dataset where each row is a separate ID   
            final_data = pd.DataFrame()
            
            # Create a full_dataset that stores all the data for each of the IDs
            complete_data = pd.DataFrame()
            for soul_num, soul in enumerate(samples_to_loop_through):
                tw = 0
                if soul_num == 0: 
                    resume = False 
                outcome, iden_data, full_data = _get_page_data(driver, soul, japanese=japanese, resume=resume)
                
                # ERROR HANDLING 
                # If the connection fails (-1) try to connect with some wait time in between. 
                # If it fails repeatedly raise an exception, close driver, and go to parent exception logic
                if outcome == -1: 
                    resume=False 
                    while outcome == -1: # start looping and waiting 
                        outcome, iden_data, full_data = _get_page_data(driver, soul, japanese=japanese, resume=resume)
                        if outcome == -1: # Again, while we keep getting connection errors wait 
                            tw+=1 
                            if tw==25: 
                                raise Exception('Continuing server error, try again')
                            # Multiply integers by 100, will plateau around 9 seconds. 
                            print('Waiting',str(np.log(tw*100)),'before reconnect attempt')
                            time.wait(np.log(tw*100))
                        # If we did no longer get a connection error we will move on to the other if statements that handle other errors

                # If the ID was not 10 digits 
                if outcome == -2 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-2]}))
                    continue 

                # If no ID number was entered
                if outcome == -3 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-3]}))
                    continue                     

                # If the ID is not in the database
                if outcome == -4 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-4]}))
                    continue                     
                        
                if outcome == 1 : 
                    final_data = final_data.append(iden_data)
                    full_data['Individual Identification  Number'] = soul 
                    complete_data = complete_data.append(full_data)
                    # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    attempts = 0 
                    
                scraped += soul
                
                if split_save > 0: 
                    if soul_num == record_number:
                        # Save the data temporarily to get it out of cahched memory and return new items
                        failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, name='temporary', return_names=True)
                        
        except Exception as e: 
            
            # If we've scraped data already then we can save it so we don't repeat it. 
            if len(complete_data)>0:
                # Save the data temporarily to get it out of cahched memory and return new items
                failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, name='temporary', return_names=True)
            
            print(e)
            try: # If we've failed and the driver is still connected then we can close it. 
                driver.quit()
            except: 
                pass

            # If attempts is at 10 attempts we will not be trying again and just return the error
            if attempts >=10: 
                print('Failed to succeed after', str(attempts), 'attempts')
                # If we're giving up then we save whatever we've scraped along with the list of IDs that has not been scraped yet
                failures, final_data, complete_data = _append_temporary_files(temp_files_saved)
                _handle_save(failures, final_data, complete_data, temp_counter, name='recovered', return_names=True)

                raise e             

            time.sleep(5) # wait 5 seconds before trying again 
                
    if split_save > 0: 
        # If there is still leftover data save it for the final append 
        if len(complete_data)>0:
            failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, name='temporary', return_names=True)
        
        failures, final_data, complete_data = _append_temporary_files(temp_files_saved)
    
    driver.quit()
    
    return final_data, complete_data, failures 
    

def connection_check(driver): 
    '''Check that the driver is still connected to the server. This can be executed before the search function is entered'''
    soup = BeautifulSoup(driver.page_source, features="lxml")

    # Detecting "Enable Cookies" is the way to tell if the connection was cut. 
    lens = [len(list(table.parents)) for table in soup.find_all('table')]
    tables = [table for table in soup.find_all('table') if len(list(table.parents))==max(lens)]
    if 'Enable Cookies' in tables[0].text.strip():
        return False
    else:
        return True 
    
    
def _search_engine_check(tables, soup, japanese):
    if japanese: 
    # The complete error message returned in that table is 
        # '要求された URL は本サーバでは見つかりませんでした。\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    もし手入力で URL を入力した場合は、綴りを確認して再度お試し下さい。\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t以下のいずれかが原因として考えられます。\n\n\nCookieを有効にしてください。\nしばらくアクセスがなかったために、牛の個体識別情報検索サーバとの接続がタイムアウトになりました。再度トップページよりアクセスください。'
        if 'Cookie' in tables[0].text.strip(): 
            print('Connection failure, try again')
            return -1, 0, 0
        
        # If an error message was returned in the japanese version it can be one of two things: 
        if len(soup.find_all('ul', {'class':'error_message'}))>0:    
            # The complete error message for entering a short number is 
            # '個体識別番号は１０桁で入力してください。'
            if '個体識別番号は１０桁で入力してください' in soup.find_all('ul', {'class':'error_message'})[0].text.strip():
                print('Not 10 digits')
                return -2, 0, 0
    
            # The complete error message for not entering a number is 
            # '個体識別番号を入力してください。\n個体識別番号は半角数字を入力してください。'
            elif '個体識別番号を入力してください' in soup.find_all('ul', {'class':'error_message'})[0].text.strip():
                print('No ID entered')
                return -3, 0, 0 
    
        # If the id was not found then the following elements should be returned. 
        if len(soup.find_all('span', {'class':'nor'}))==4: 
            if '入力された個体識別番号をお確かめくださいますようお願いします' in soup.find_all('span', {'class':'nor'})[2].text.strip():
                return -4, 0, 0
    
    else: 
        # These are the types of problem pages that can be returned while searching. 
        if 'Enable Cookies' in tables[0].text.strip():
            print('Connection failure, try again')
            return -1, 0, 0
        elif 'The Individual Identification Number needs to be ten figures' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('Not 10 digits')
            return -2, 0, 0 
        elif 'Input the Individual Identification Number' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('No ID entered')
            return -3, 0, 0 
        elif 'there are no cattle' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('No cattle found')
            return -4, 0, 0

    
def _get_page_data(driver, soul, japanese=True, resume=False):
    '''Get the data off of an id.nlbc.go.jp page
    __________
    parameters
    - driver : selenium webdriver object
    - soul : int, id number 
    '''
    
    if not resume: 
        if japanese:
            # Japanese Version 
            driver.get('https://www.id.nlbc.go.jp/CattleSearch/search/agreement')
        else: 
            # Go to the website 
            driver.get('https://www.id.nlbc.go.jp/CattleSearch/search/agreement_en')
            
        # Check that the connection is live
        connected = connection_check(driver)
    
    # If we're already connected 
    elif resume: 
        connected=True

    if not connected: # If we're not connected return non-connection error 
        return -1, 0, 0 
        
    elif connected: 
        # The first input tag link is the agree button 
        driver.find_elements_by_tag_name('input')[0].click()
        # Find the text box and enter the cattle's name 
        ele = driver.find_elements_by_name('txtIDNO')[0]
        ele.send_keys(str(soul))

        if japanese: 
            # Japanese Version 
            driver.find_elements_by_name('method:doSearch')[0].click()
        else: 
            driver.find_elements_by_name('method:doSearchEn')[0].click()

        soup = BeautifulSoup(driver.page_source)

        # Detecting "Enable Cookies" is the way to tell if the connection was cut. 
        lens = [len(list(table.parents)) for table in soup.find_all('table')]
        tables = [table for table in soup.find_all('table') if len(list(table.parents))==max(lens)]

        if len(tables)==1: 
            # If there was only one table object on the page its because something went wrong 
            # e1 codes: -1 = Connection, -2 = Short digit, -3 = No ID entered, -4 = ID not found
            # e2 and e3 are always 0. 
            e1, e2, e3 = _search_engine_check(tables, soup, japanese)
            return e1, e2, e3 

        else:
            iden_table = tables[0]
            data_table = tables[1] 

            data = {'id0':[],'id1':[],'id2':[]}
            for idx, row in enumerate(iden_table.find_all('tr')): 
                if idx == 0: # in the first row or header
                    for col in row.find_all('th'): # find every 'th' header (column) in the row
                        data[col.text]=[] # keep the headers for the column name in the dict that will be our data
                    column_names = list(data.keys())
                else:
                    for col_num, col_val in enumerate(row.find_all('td')):
                        if col_num==0:
                            for i, v in enumerate(col_val.find_all('span')): 
                                data['id'+str(i)]=str(v.text.strip())   
                            data[column_names[i+1]].append(col_val.text.strip())
                        else:
                            col_name = col_num+3 #since we added the id variables
                            data[column_names[col_name]].append(col_val.text.strip())
            iden_data = pd.DataFrame(data)   

            data = {} 
            for idx, row in enumerate(data_table.find_all('tr')): 

                if idx == 0: # in the first row or header
                    for col in row.find_all('th'): # find every 'th' header (column) in the row
                        # If we're scraping the japanese site 
                        if japanese: 
                            if '飼養施設所在地' in col.text.strip():
                                data['都道府県']=[]
                                data['市区町村']=[]
                            else:
                                data[col.text.strip()]=[] # keep the headers for the column name in the dict that will be our data
                        # If we're scraping the english site
                        else: 
                            if 'prefecture' in col.text.strip():
                                data['The prefecture']=[]
                                data['The location of the raising facility']=[]
                            else:
                                data[col.text.strip()]=[] # keep the headers for the column name in the dict that will be our data

                    column_names = list(data.keys())
                else:
                    for col_num, col_val in enumerate(row.find_all('td')):
                            data[column_names[col_num]].append(col_val.text.strip())
            full_data = pd.DataFrame(data)

            return 1, iden_data, full_data