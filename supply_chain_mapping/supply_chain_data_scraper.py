import os
from path import Path 

root = str(Path(os.path.abspath(os.path.dirname(__file__))).parent)

import time, copy, glob
import numpy as np 
import pandas as pd

from vcue import web, basics
from bs4 import BeautifulSoup
from IPython.display import clear_output

from supply_chain_mapping import random_id_generator as rig



def check_collected_data(main_list, save=True, remove_temporary=True):
    '''Check how many samples have already been collected
    __________
    parameters:
        - main_list : the list to check against 
        - save : bool. If True, saves all the data in the temporary folder to the collected folder 
        - remove_temporary : bool. If True, erases the data in the temporary folder only if save == True 
    ''' 
    complete_data, final_data, failed_data, uncollected_sample = collect_temporary(
            check_against=main_list,  # Check which ids in the original sample have not been collected
            return_data = True)                              # Get all the collected data
    
    if save: 
        collected_data_path = root+'/data/collected/'
        _handle_save(failed_data, final_data, complete_data, '', name='collected', file_path=collected_data_path)
    
        if remove_temporary: 
            clean_temporary_directory()        

    return uncollected_sample


def clean_temporary_directory(directory=''):
    if directory=='':
        directory=root+'/data/temporary/'
    
    files = glob.glob(directory+'/*.csv')
    for f in files: os.remove(f)


def collect_temporary(root_directory='', check_against=None, return_data=False):
    '''Gets the ID data in the temporary folder and appends it to the data in the collected folder
    __________
    parameters:
        - root_directory : str. Directory where the "/temporary" and "/collected" data folders can be found.  
        - check_against : list of str. A list of IDs to check against, this will tell you how many IDs in this list have already been scraped. Returns "difference" list of uncollected IDs 
        - return_data : bool. If True, returns a tuple with three dataframe objects (complete_data, final_data, failed_data) [for saving the data], if check_against is not None returns (complete_data, final_data, failed_data, difference) 
    '''
    if return_data:
        # Get the temporary IDs data
        temp_collected, temp_failed, temp_complete_data, temp_final_data, temp_failed_data = check_ids(directory=root_directory, temporary=True, return_data=True)    
        # Get the collected IDs data
        collected, failed, complete_data, final_data, failed_data = check_ids(directory=root_directory, temporary=False, return_data=True)        
    else: 
        # Get the temporary IDs
        temp_collected, temp_failed = check_ids(directory=root_directory, temporary=True, return_data=False)    
        # Get the collected IDs
        collected, failed = check_ids(directory=root_directory, temporary=False, return_data=False)        
    
    if check_against is not None: 
        assert type(check_against)==list
        
        setz = set(list(temp_collected) + list(collected))
        sety = set(check_against)
                
        difference = sety.difference(setz) # Remove the collected IDs from the check_against list
        print(str(len(difference)),'IDs in the submitted list have not been collected')
    
    # Calculate the number of new IDs in the temporary folder
    seta = set(temp_collected)
    setb = set(collected)
    newids = seta.difference(setb) 
    
    # Append the datasets 
    complete_data = complete_data.append(temp_complete_data).drop_duplicates()
    final_data = final_data.append(temp_final_data).drop_duplicates()
    failed_data = failed_data.append(temp_failed_data).drop_duplicates()
    
    print(str(len(newids)), 'new IDs in the temporary folder have been appended to', str((len(collected))), 'IDs in the collected folder')
    print('There are a total of', str(len(final_data)),' collected IDs')
    print('There are a total of', str(len(failed_data)),'failed IDs')

    if return_data:
        if check_against is not None: 
            return complete_data, final_data, failed_data, list(difference) 
        else:        
            return complete_data, final_data, failed_data
    else: 
        if check_against is not None:
            return list(difference)


def check_ids(directory='', temporary=True, return_data=False):
    '''Checks which IDs are in the given folder
    ___________
    parameters:
        - directory : str. directory to check IDs
        - temporary : bool. If directory is "" then if True, check the temporary data folder, otherwise the collected
    '''
    if directory=='':
        if temporary:
            directory=root+'/data/temporary/'
        else: 
            directory=root+'/data/collected/'
    
    files = glob.glob(directory+'/*.csv')
    final_files = [f for f in files if 'final_' in f]
    failure_files = [f for f in files if 'failure_' in f]

    # If we want to hold on to the data
    if return_data:
        final_data = pd.DataFrame()
        failed_data = pd.DataFrame() 
        
        # We will want the complete data as well (IDs are covered in final)
        complete_data = pd.DataFrame()
        complete_files = [f for f in files if 'complete_' in f]
        
        for f in complete_files: 
            dta = pd.read_csv(f, dtype={'Individual Identification  Number': object})
            complete_data = complete_data.append(dta)
    
    # Get the final IDs and data if needed 
    collected = []
    for f in final_files: 
        dta = pd.read_csv(f, dtype={'個体識別番号': object})
        if '個体識別番号' in dta: 
            collected += list(dta['個体識別番号'].values)
        if return_data:
            final_data = final_data.append(dta)

    # Get the failed IDs and failed data if needed
    failed = []
    for f in failure_files: 
        dta = pd.read_csv(f, dtype={'ID': object})
        failed += list(dta['ID'].values)
        if return_data:
            failed_data = failed_data.append(dta)
            
    if return_data: 
        # Return the data without the default row id columns imported by the csv format 
        complete_data = complete_data[[c for c in complete_data.columns if 'Unnamed' not in c]]
        failed_data = failed_data[[c for c in failed_data.columns if 'Unnamed' not in c]]
        final_data = final_data[[c for c in final_data.columns if 'Unnamed' not in c]]        
        return collected, failed, complete_data, final_data, failed_data
    else:              
        return collected, failed


def quickload_samples(original_data_dir=root+'/data/original/'):
    
    # These sample IDs were collected from photos of a supermarket in Tokyo 
    sample = pd.read_csv(original_data_dir+'sample_tokyo_sm.csv')
    
    # Market IDs start with "1", test IDs start with "0" but are still valid. 
    # These IDs should be complete so if any are short they likely need a "0" added in front. 
    
    sample['id'] = sample.astype(str)          # Make the id column a string
    sample['len'] = sample['id'].str.len()     # Get their length 
    sample['zeros'] = 10 - sample['len']       # How many characters are missing to get to a full ID 
    for i in list(sample['zeros'].unique()):   # Add as many 0's as necessary in front of each ID (this should not exceed 1)
        sample.loc[sample['zeros']==i, 'id'] = ('0'*i) + sample.loc[sample['zeros']==i, 'id']    
    sample = sample.drop(['len','zeros'],1)    # Drop the working variables
    
    sample_tokyo_sm = sample 
    sample_tokyo_sm.tail(3)
    
    sample = pd.read_csv(original_data_dir+'sample_fukushima.csv')
    
    sample['id'] = sample.astype(str)          # Make the id column a string
    sample['len'] = sample['id'].str.len()     # Get their length 
    sample['first_digit'] = sample['id'].str[0]
    sample['zeros'] = 10 - sample['len']       # How many characters are missing to get to a full ID 
    
    sample[sample['len']==9]
    
    sample['corrected_id'] = ''
    # If the sample started with a 1 then it was an official ID and needs to have the last digit added. 
    sample.loc[(sample['first_digit']=='1') & (sample['len']==9), 'corrected_id'] = sample.loc[(sample['first_digit']=='1') & (sample['len']==9), 'id'].apply(lambda d: d + str(rig.get_last_digit(d)))
    
    # If the sample started with anything other than a 1 it was probably a test ID and it just needs a 0 attached to the front 
    sample.loc[(sample['first_digit']!='1') & (sample['len']==9), 'corrected_id'] = "0" + sample.loc[(sample['first_digit']!='1') & (sample['len']==9), 'id']
    sample.loc[(sample['corrected_id']==''), 'corrected_id'] = sample.loc[(sample['corrected_id']==''), 'id']
    
    # Replace the original id with the corrected id (optional)
    sample['id'] = sample['corrected_id']
    sample = sample[['id']]
    sample_fukushima = sample
    sample_fukushima.tail(3)
    
    sample1 = pd.read_csv(original_data_dir+'recalled_raw_p1.csv', header=None)
    ids = list(sample1[0].values)
    sample2 = pd.read_csv(original_data_dir+'recalled_raw_p2.csv', header=None, dtype={'ID': object})
    for c in sample2.columns: 
        ids += list(sample2[c].values) 
        
    sample = pd.DataFrame({'id':ids}).dropna()
    sample['id'] = sample['id'].str.encode('ascii', 'ignore').str.decode('ascii')
    sample['len'] = sample['id'].str.len()
    
    sample = sample.dropna()
    sample = sample[(sample['len']==9) | (sample['len']==10)]
    
    sample['first_digit'] = sample['id'].str[0]
    
    sample['corrected_id'] = ''
    # If the sample started with a 1 then it was an official ID and needs to have the last digit added. 
    sample.loc[(sample['first_digit']=='1') & (sample['len']==9), 'corrected_id'] = sample.loc[(sample['first_digit']=='1') & (sample['len']==9), 'id'].apply(lambda d: d + str(rig.get_last_digit(d)))
    
    # If the sample started with anything other than a 1 it was probably a test ID and it just needs a 0 attached to the front 
    sample.loc[(sample['first_digit']!='1') & (sample['len']==9), 'corrected_id'] = "0" + sample.loc[(sample['first_digit']!='1') & (sample['len']==9), 'id']
    sample.loc[(sample['corrected_id']==''), 'corrected_id'] = sample.loc[(sample['corrected_id']==''), 'id']
    
    # Replace the original id with the corrected id (optional)
    sample['id'] = sample['corrected_id']
    sample = sample[['id']]
    sample_recall = sample
    sample_recall.tail(3)
    
    return sample_recall, sample_fukushima, sample_tokyo_sm


def _handle_save(failures, final_data, complete_data, _saves, name='recovery', file_path=root,return_names=False):
    '''A short internal function to save files'''
    failures.to_csv(file_path+'/failure_'+name+'_save'+str(_saves)+'.csv')
    final_data.to_csv(file_path+'/final_'+name+'_save'+str(_saves)+'.csv')
    complete_data.to_csv(file_path+'/complete_'+name+'_save'+str(_saves)+'.csv')
    if return_names: 
        return file_path+'/failure_'+name+'_save'+str(_saves)+'.csv', file_path+'/final_'+name+'_save'+str(_saves)+'.csv', file_path+'/complete_'+name+'_save'+str(_saves)+'.csv'
    

def _append_temporary_files(filename_tuple_list):
    '''Append and accumulate all the temporary data files'''
    failures, final_data, complete_data = pd.DataFrame({'ID':[],'Failure Code':[]}), pd.DataFrame(), pd.DataFrame()

    for f_tuple in filename_tuple_list: 
        f1, f2, f3 = f_tuple
        
        d1 = pd.read_csv(f1) # failure 
        d2 = pd.read_csv(f2) # final 
        d3 = pd.read_csv(f3) # complete 

        failures = failures.append(d1)        
        final_data = final_data.append(d2)
        complete_data = complete_data.append(d3)

    return failures, final_data, complete_data        

    
def _remove_scraped(master_list, scraped_list):
    '''Remove items in the scraped_list from the master list'''
    seta = set(master_list)
    setb = set(scraped_list)
    cleaned = seta.difference(setb)
    return list(cleaned)


def _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, file_path=root, name='temporary', return_names=True):
    '''An internal function to save temporary files and remove them from an on-going list'''
    # Save the data so far in a temporary file and add it to a list of files so they can be appended again later
    temp_files_saved += _handle_save(failures, final_data, complete_data, temp_counter, name='temporary', file_path=file_path, return_names=True)

    # Reset all the datasets so we can clear cached memory 
    failures, final_data, complete_data = pd.DataFrame({'ID':[],'Failure Code':[]}), pd.DataFrame(), pd.DataFrame()
    temp_counter += 1 

    # Remove the IDs that have been scraped so we don't repeat work
    master_list = _remove_scraped(master_list, scraped)                    
    
    return failures, final_data, complete_data, temp_counter, master_list


def supply_chain_data_scraper(samples : list, japanese = True, split_save = 0, temporary_file_path=root, final_file_path=root):
    '''Submit a list of numbers (ids) that can be used to identify cows in on the site "https://www.id.nlbc.go.jp/CattleSearch/search/agreement_en"
    __________
    parameters
    - samples : list of str. Each element should be an ID to be scraped. 
    - japanese : bool, default True. If true scrape the japanese website, if false use the english site. The English site displays less detailed information. 
    - split_save : int. Save the data in progress after this many ids have been scraped. This should be a rather high number, the point at which holding the data in memory becomes expensive in terms of time 
    '''
    
    # Create a deep copy of the master list because we might be forced to modify it and we don't want to affect the original 
    master_list = copy.deepcopy(samples)    
    total_samples = len(master_list)
    samples_to_loop_through = copy.deepcopy(samples)  # Create a live list of the samples we need to loop through at any given time 
    scraped = [] # Create a list to store the samples we have scraped (and found or not found)
    
    display_counter = 0
    last_error = None  # for reporting errors without crashing 

    # Temporary stored data counter, used for files that are recovered after and error and files that are saved using the split_save functionality 
    temp_counter = 1 

    # If the scraper fails completely 10 times then close it and return the error 
    attempts = 0 

    if split_save > 0: 
        record_number = split_save
        temp_files_saved = []

    complete = False

    scraped = []    

    while attempts < 10 or not complete: 
        attempts +=1 
        try: 
            # Launch a Selenium web driver 
            driver = web.launch_driver()
            
            # Create a dataset to record failures and store them separately from the data
            failures = pd.DataFrame({'ID':[],'Failure Code':[]}) 

            # Create a dataset to store the summary dataset where each row is a separate ID   
            final_data = pd.DataFrame()
            
            # Create a full_dataset that stores all the data for each of the IDs
            complete_data = pd.DataFrame()
            
            # Before engaging in the loop, remove the IDs that were scraped
            set1 = set(samples_to_loop_through)
            set2 = set(scraped)
            samples_to_loop_through = list(set1.difference(set2)) 
            
            for soul_num, soul in enumerate(samples_to_loop_through):
                
                clear_output()
                if last_error is not None: 
                    print('Last error was:')
                    print(last_error)
                print('Scraped',len(list(scraped)), 'IDs from the',total_samples,'originally submitted')   
                print('Current loop is working on the', len(samples_to_loop_through), 'remaining IDs')
                print('Working on ID #', str(display_counter), 'of the current loop')
                
                tw = 0
                if soul_num == 0: 
                    resume = False 
                outcome, iden_data, full_data = _get_page_data(driver, soul, japanese=japanese, resume=resume)
                
                # ERROR HANDLING 
                # If the connection fails (-1) try to connect with some wait time in between. 
                # If it fails repeatedly raise an exception, close driver, and go to parent exception logic
                if outcome == -1: 
                    resume=False 
                    while outcome == -1: # start looping and waiting 
                        outcome, iden_data, full_data = _get_page_data(driver, soul, japanese=japanese, resume=resume)
                        if outcome == -1: # Again, while we keep getting connection errors wait 
                            tw+=1 
                            if tw==25: 
                                raise Exception('Continuing server error, try again')
                            # Multiply integers by 100, will plateau around 9 seconds. 
                            print('Waiting',str(np.log(tw*100)),'before reconnect attempt')
                            time.wait(np.log(tw*100)*2)
                        # If we did no longer get a connection error we will move on to the other if statements that handle other errors

                # If the ID was not 10 digits 
                if outcome == -2 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-2]}))
                    display_counter += 1 
                    scraped.append(soul)
                    continue 

                # If no ID number was entered
                if outcome == -3 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-3]}))
                    display_counter += 1 
                    scraped.append(soul)
                    continue                     

                # If the ID is not in the database
                if outcome == -4 : 
                    attempts = 0 # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    failures = failures.append(pd.DataFrame({'ID':[soul],'Failure Code':[-4]}))
                    display_counter += 1 
                    scraped.append(soul)
                    continue                     
                        
                if outcome == 1 : 
                    final_data = final_data.append(iden_data)
                    full_data['Individual Identification  Number'] = soul 
                    complete_data = complete_data.append(full_data)
                    # If we succeed on an attempt then we start the attempts again (Note this is only a counter for script errors)
                    attempts = 0 
                    display_counter += 1 
                    scraped.append(soul)
                
                if split_save > 0: 
                    if soul_num == record_number:
                        # Save the data temporarily to get it out of cahched memory and return new items
                        failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, file_path=temporary_file_path, name='temporary', return_names=True)
                    

                # If we've gotten through a whole cycle we can just continue on the current window, no need to reload the URL. 
                resume = True
                
            complete = True 
         
        except KeyboardInterrupt:

            # If we've scraped data already then we can save it so we don't repeat it. 
            if len(complete_data)>0:
                # Save the data temporarily to get it out of cahched memory and return new items
                failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, file_path=temporary_file_path, name='temporary', return_names=True)

            failures, final_data, complete_data = _append_temporary_files(temp_files_saved)
            _handle_save(failures, final_data, complete_data, temp_counter, file_path=temporary_file_path, name='recovered', return_names=True)
            basics.full_pickle(temporary_file_path+'master_list', master_list)            
            
            raise       
         
        except Exception as e: 
            
            # If we've scraped data already then we can save it so we don't repeat it. 
            if len(complete_data)>0:
                # Save the data temporarily to get it out of cahched memory and return new items
                failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, file_path=temporary_file_path, name='temporary', return_names=True)
            
            print(e)
            last_error = e
            try: # If we've failed and the driver is still connected then we can close it. 
                driver.quit()
            except: 
                pass

            # If attempts is at 10 attempts we will not be trying again and just return the error
            if attempts >=10: 
                print('Failed to succeed after', str(attempts), 'attempts')
                # If we're giving up then we save whatever we've scraped along with the list of IDs that has not been scraped yet
                failures, final_data, complete_data = _append_temporary_files(temp_files_saved)
                _handle_save(failures, final_data, complete_data, temp_counter, file_path=final_file_path, name='recovered', return_names=True)
                basics.full_pickle(temporary_file_path+'master_list', master_list)
                raise e             

            time.sleep(5) # wait 5 seconds before trying again 
                
    if split_save > 0: 
        # If there is still leftover data save it for the final append 
        if len(complete_data)>0:
            failures, final_data, complete_data, temp_counter, master_list = _checkpoint(failures, final_data, complete_data, temp_counter, temp_files_saved, master_list, scraped, file_path=temporary_file_path, name='temporary', return_names=True)
        
        failures, final_data, complete_data = _append_temporary_files(temp_files_saved)
        _handle_save(failures, final_data, complete_data, temp_counter, file_path=final_file_path, name='finalized', return_names=True)
    
    driver.quit()
    
    return final_data, complete_data, failures 
    

def connection_check(driver): 
    '''Check that the driver is still connected to the server. This can be executed before the search function is entered'''
    soup = BeautifulSoup(driver.page_source, features="lxml")

    # Detecting "Enable Cookies" is the way to tell if the connection was cut. 
    lens = [len(list(table.parents)) for table in soup.find_all('table')]
    tables = [table for table in soup.find_all('table') if len(list(table.parents))==max(lens)]
    if 'Enable Cookies' in tables[0].text.strip():
        return False
    else:
        return True 
    
    
def _search_engine_check(tables, soup, japanese):
    if japanese: 
    # The complete error message returned in that table is 
        # '要求された URL は本サーバでは見つかりませんでした。\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    もし手入力で URL を入力した場合は、綴りを確認して再度お試し下さい。\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t以下のいずれかが原因として考えられます。\n\n\nCookieを有効にしてください。\nしばらくアクセスがなかったために、牛の個体識別情報検索サーバとの接続がタイムアウトになりました。再度トップページよりアクセスください。'
        if 'Cookie' in tables[0].text.strip(): 
            print('Connection failure, try again')
            return -1, 0, 0
        
        # If an error message was returned in the japanese version it can be one of two things: 
        if len(soup.find_all('ul', {'class':'error_message'}))>0:    
            # The complete error message for entering a short number is 
            # '個体識別番号は１０桁で入力してください。'
            if '個体識別番号は１０桁で入力してください' in soup.find_all('ul', {'class':'error_message'})[0].text.strip():
                print('Not 10 digits')
                return -2, 0, 0
    
            # The complete error message for not entering a number is 
            # '個体識別番号を入力してください。\n個体識別番号は半角数字を入力してください。'
            elif '個体識別番号を入力してください' in soup.find_all('ul', {'class':'error_message'})[0].text.strip():
                print('No ID entered')
                return -3, 0, 0 
    
        # If the id was not found then the following elements should be returned. 
        if len(soup.find_all('span', {'class':'nor'}))==4: 
            if '入力された個体識別番号をお確かめくださいますようお願いします' in soup.find_all('span', {'class':'nor'})[2].text.strip():
                return -4, 0, 0
    
    else: 
        # These are the types of problem pages that can be returned while searching. 
        if 'Enable Cookies' in tables[0].text.strip():
            print('Connection failure, try again')
            return -1, 0, 0
        elif 'The Individual Identification Number needs to be ten figures' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('Not 10 digits')
            return -2, 0, 0 
        elif 'Input the Individual Identification Number' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('No ID entered')
            return -3, 0, 0 
        elif 'there are no cattle' in soup.find_all('span', {'class':'nor'})[1].text.strip():
            print('No cattle found')
            return -4, 0, 0

    
def _get_page_data(driver, soul, japanese=True, resume=False):
    '''Get the data off of an id.nlbc.go.jp page
    __________
    parameters
    - driver : selenium webdriver object
    - soul : int, id number 
    '''
    
    if not resume: 
        if japanese:
            # Japanese Version 
            driver.get('https://www.id.nlbc.go.jp/CattleSearch/search/agreement')
        else: 
            # Go to the website 
            driver.get('https://www.id.nlbc.go.jp/CattleSearch/search/agreement_en')
            
        # Check that the connection is live
        connected = connection_check(driver)
    
    # If we're already connected 
    elif resume: 
        connected=True

    if not connected: # If we're not connected return non-connection error 
        return -1, 0, 0 
        
    elif connected: 
        # The first input tag link is the agree button 
        driver.find_elements_by_tag_name('input')[0].click()
        # Find the text box and enter the cattle's name 
        ele = driver.find_elements_by_name('txtIDNO')[0]
        ele.send_keys(str(soul))

        if japanese: 
            # Japanese Version 
            driver.find_elements_by_name('method:doSearch')[0].click()
        else: 
            driver.find_elements_by_name('method:doSearchEn')[0].click()

        soup = BeautifulSoup(driver.page_source)

        # Detecting "Enable Cookies" is the way to tell if the connection was cut. 
        lens = [len(list(table.parents)) for table in soup.find_all('table')]
        tables = [table for table in soup.find_all('table') if len(list(table.parents))==max(lens)]

        if len(tables)==1: 
            # If there was only one table object on the page its because something went wrong 
            # e1 codes: -1 = Connection, -2 = Short digit, -3 = No ID entered, -4 = ID not found
            # e2 and e3 are always 0. 
            e1, e2, e3 = _search_engine_check(tables, soup, japanese)
            return e1, e2, e3 

        else:
            iden_table = tables[0]
            data_table = tables[1] 

            data = {'id0':[],'id1':[],'id2':[]}
            for idx, row in enumerate(iden_table.find_all('tr')): 
                if idx == 0: # in the first row or header
                    for col in row.find_all('th'): # find every 'th' header (column) in the row
                        data[col.text]=[] # keep the headers for the column name in the dict that will be our data
                    column_names = list(data.keys())
                else:
                    for col_num, col_val in enumerate(row.find_all('td')):
                        if col_num==0:
                            for i, v in enumerate(col_val.find_all('span')): 
                                data['id'+str(i)]=str(v.text.strip())   
                            data[column_names[i+1]].append(col_val.text.strip())
                        else:
                            col_name = col_num+3 #since we added the id variables
                            data[column_names[col_name]].append(col_val.text.strip())
            iden_data = pd.DataFrame(data)   

            data = {} 
            for idx, row in enumerate(data_table.find_all('tr')): 

                if idx == 0: # in the first row or header
                    for col in row.find_all('th'): # find every 'th' header (column) in the row
                        # If we're scraping the japanese site 
                        if japanese: 
                            if '飼養施設所在地' in col.text.strip():
                                data['都道府県']=[]
                                data['市区町村']=[]
                            else:
                                data[col.text.strip()]=[] # keep the headers for the column name in the dict that will be our data
                        # If we're scraping the english site
                        else: 
                            if 'prefecture' in col.text.strip():
                                data['The prefecture']=[]
                                data['The location of the raising facility']=[]
                            else:
                                data[col.text.strip()]=[] # keep the headers for the column name in the dict that will be our data

                    column_names = list(data.keys())
                else:
                    for col_num, col_val in enumerate(row.find_all('td')):
                            data[column_names[col_num]].append(col_val.text.strip())
            full_data = pd.DataFrame(data)

            return 1, iden_data, full_data